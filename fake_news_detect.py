# -*- coding: utf-8 -*-
"""fake_news_detect.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FRDqLY9G0LO7nxn5dDSDSEGwwMg472_U
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
import re
import string

from google.colab import drive
drive.mount('/content/drive')

df_fake = pd.read_csv("/content/drive/My Drive/Colab Notebooks/Fake.csv")
df_true = pd.read_csv("/content/drive/My Drive/Colab Notebooks/True.csv")

df_true

df_fake

df_fake["label"] = 0
df_true["label"] = 1
df = pd.concat([df_fake, df_true], axis =0 )

df = df.drop_duplicates(subset=['title'])
df = df.drop(["subject","date","text"], axis = 1)
df

df = df.sample(frac=1, random_state=42).reset_index(drop=True)
df

def count_words(text):
    words = re.findall(r'\b\w+\b', text)
    return len(words)
df['len']=df["title"].apply(count_words)
df.describe()

def wordopt(text):
    text = text.lower()
     #BOOM! Watch VIDEO  video!
    text = re.sub('boom!', '', text)
    text = re.sub('watch video', '', text)
    text = re.sub('video!', '', text)

    text = re.sub('wow', '', text)
    text = re.sub('wow:', '', text)
    text = re.sub('wow!', '', text)
    text = re.sub('watch:', '', text)
    text = re.sub('breaking:', '', text)
    text = re.sub('just in:', '', text)
    text = re.sub('report:', '', text)
    text = re.sub('breaking news:', '', text)
    text = re.sub('flashback:', '', text)
    text = re.sub('busted:', '', text)
    text = re.sub('bombshell:', '', text)
    text = re.sub('sunday screening:', '', text)
    text = re.sub('update:', '', text)
    text = re.sub('revealed:', '', text)
    text = re.sub('not kidding:', '', text)

    text = re.sub('video:', '', text)
    text = re.sub('breaking bombshell:', '', text)
    text = re.sub('factbox:', '', text)
    text = re.sub('exclusive:', '', text)
    text = re.sub('highlights:', '', text)
    text = re.sub('timeline:', '', text)
    text = re.sub('instant view:', '', text)

    text = re.sub(r'^watch', '', text)
    text = re.sub(r'^shocking', '', text)



    text=re.sub(r'\S*/\S*', '', text)
    text = re.sub('\(.*?\)', '', text)
    text = re.sub(r'^.*?watch:', '', text)
    text=re.sub(r'^.*?report:', '', text)
    text=re.sub(r'^.*?breaking:', '', text)
    text = re.sub('\[.*?\]', '', text)
    #text = re.sub("\\W"," ",text)
    text = re.sub('https?://\S+|www\.\S+', '', text)  #????
    text = re.sub('<.*?>+', '', text)
    #text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    return text
df['title'] = df['title'].apply(wordopt)

def clear_text(t):
    t = t.lower()
    t = re.sub('\[.*?\]', '', t)
    t = re.sub('https?://\S+|www\.\S+', '', t)
    t = re.sub('<.*?>+', '', t)
    t = re.sub('\w*\d\w*', '', t)
    t = re.sub('[%s]' % re.escape(string.punctuation),
    '', t)
    t = re.sub('\n', ' ', t)
    t = re.sub('\s+', ' ', t).strip()
    return t
df['title'] = df['title'].apply(clear_text)

from sklearn.model_selection import train_test_split

x_temp, x_test, y_temp, y_test = train_test_split(
    df['title'], df['label'],
    test_size=0.2,
    stratify=df['label'],
    random_state=42
)
x_train, x_val, y_train, y_val = train_test_split(
    x_temp, y_temp,
    test_size=0.25,
    stratify=y_temp,
    random_state=42
)

print(f"Train: {len(x_train)}")
print(f"Validation: {len(x_val)}")
print(f"Test: {len(x_test)}")

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

df_plot = pd.DataFrame({

    'train': pd.Series(y_train),
    'validation': pd.Series(y_val),
    'test': pd.Series(y_test),

})
df_melted = df_plot.melt(var_name='Split', value_name='Label')

plt.figure(figsize=(9, 5))
sns.set_style("whitegrid")
sns.set_palette("pastel")
sns.countplot(data=df_melted, x='Split', hue='Label')

plt.ylabel("Количество новостей")
plt.xlabel("Выборка")
plt.legend(title="Класс", loc="upper right")
plt.tight_layout()
plt.show()

count_fake = (df['label'] == 0).sum()
count_fake

count_fake = (df['label'] == 1).sum()
count_fake

import nltk
nltk.download('stopwords')

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import text
from nltk.corpus import stopwords
stop_words = text.ENGLISH_STOP_WORDS
vectorizer = TfidfVectorizer(stop_words=list(stopwords.words('english')), max_df=0.9, min_df=5)
xv_train = vectorizer.fit_transform(x_train)
xv_test = vectorizer.transform(x_test)

from sklearn.linear_model import LogisticRegression

LR = LogisticRegression()
LR.fit(xv_train,y_train)
pred_lr=LR.predict(xv_test)
LR.score(xv_test, y_test)

from sklearn.metrics import confusion_matrix, classification_report,ConfusionMatrixDisplay
import seaborn as sns
import matplotlib.pyplot as plt


cm = confusion_matrix(y_test, pred_lr)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)

disp.plot(cmap='Blues')
plt.title("LogisticRegression")
plt.show()
print("\nClassification Report for LogisticRegression")
print(classification_report(y_test, pred_lr,digits=4))

feature_names = vectorizer.get_feature_names_out()
coefficients = LR.coef_[0]

top_fake_idx = np.argsort(coefficients)[-20:]
top_real_idx = np.argsort(coefficients)[:20]

print("\nTop 20 слов, указывающих на REAL:")
for idx in reversed(top_fake_idx):
    print(f"{feature_names[idx]}: {coefficients[idx]:.4f}")

print("\nTop 20 слов, указывающих на FAKE:")
for idx in top_real_idx:
    print(f"{feature_names[idx]}: {coefficients[idx]:.4f}")

import numpy as np
import matplotlib.pyplot as plt

top_fake_idx = np.argsort(coefficients)[-20:]
top_real_idx = np.argsort(coefficients)[:20]

top_fake_words = [(feature_names[i], coefficients[i]) for i in top_fake_idx]
top_real_words = [(feature_names[i], coefficients[i]) for i in top_real_idx]

words = [w for w, _ in reversed(top_fake_words)] + [w for w, _ in top_real_words]
values = [v for _, v in reversed(top_fake_words)] + [v for _, v in top_real_words]

colors = ['mediumseagreen'] * 20 + ['tomato'] * 20
plt.figure(figsize=(12, 10))
plt.barh(words, values, color=colors)
plt.axvline(0, color='black', linewidth=0.8)
plt.xlabel('Вес слова')
plt.ylabel('Слово')
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
top_real_idx= np.argsort(coefficients)[-20:]
top_fake_idx = np.argsort(coefficients)[:20]

top_fake_words = [(feature_names[i], coefficients[i]) for i in top_fake_idx]
top_real_words = [(feature_names[i], coefficients[i]) for i in top_real_idx]

plt.figure(figsize=(10, 6))
words_fake = [w for w, _ in reversed(top_fake_words)]
values_fake = [v for _, v in reversed(top_fake_words)]
plt.barh(words_fake, values_fake, color='tomato')
plt.title('Топ 20 слов, указывающих на FAKE')
plt.xlabel('Вес слова')
plt.ylabel('Слово')
plt.axvline(0, color='black', linewidth=0.8)
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
words_real = [w for w, _ in top_real_words]
values_real = [v for _, v in top_real_words]
plt.barh(words_real, values_real, color='mediumseagreen')
plt.title('Топ 20 слов, указывающих на REAL')
plt.xlabel('Вес слова')
plt.ylabel('Слово')
plt.axvline(0, color='black', linewidth=0.8)
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter
import re

def extract_top_colon_starters(df, top_n=15):
    mask = df['title'].str.contains(':', case=False, na=False)
    titles_with_colon = df[mask]['title']

    def extract_before_colon(text):
        match = re.findall(r'^([^:]+):', text)
        return [m.strip().lower() for m in match]

    all_words = []
    for title in titles_with_colon:
        words = extract_before_colon(title)
        all_words.extend(words)

    return Counter(all_words).most_common(top_n)

top_fake = extract_top_colon_starters(df_fake, top_n=15)
top_true = extract_top_colon_starters(df_true, top_n=15)
df_plot = pd.DataFrame({
    'word': [w for w, _ in top_fake + top_true],
    'count': [c for _, c in top_fake + top_true],
    'dataset': ['FAKE'] * len(top_fake) + ['REAL'] * len(top_true)
})

df_plot = df_plot.drop_duplicates(subset=['word', 'dataset'])

plt.figure(figsize=(14, 7))
colors = {'FAKE': 'tomato', 'REAL': 'mediumseagreen'}

for label in df_plot['dataset'].unique():
    subset = df_plot[df_plot['dataset'] == label]
    bars = plt.bar(subset['word'], subset['count'], label=label, alpha=0.8, color=colors[label])
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, height + 0.5, str(int(height)),
                 ha='center', va='bottom', fontsize=9)


plt.title('Топ 15 подслов перед ":" ', fontsize=14)
plt.xlabel('Подслово перед ":"', fontsize=12)
plt.ylabel('Частота появления', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

result = df[df['title'].str.contains('shocking', case=False, na=False)]['title']

print(result)



df.iloc[38518]['title']

from sklearn.tree import DecisionTreeClassifier

DT = DecisionTreeClassifier()
DT.fit(xv_train, y_train)

pred_dt = DT.predict(xv_test)
DT.score(xv_test, y_test)

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 10))
plot_tree(DT, filled=True, feature_names=vectorizer.get_feature_names_out(), class_names=['Fake', 'Real'], max_depth=3)
plt.show()

import pandas as pd
import numpy as np

feature_importances = DT.feature_importances_
top_n = 20
top_features_idx = np.argsort(feature_importances)[::-1][:top_n]
top_features = np.array(vectorizer.get_feature_names_out())[top_features_idx]

df_importance = pd.DataFrame({
    'Слово': top_features,
    'Важность': feature_importances[top_features_idx]
})
print(df_importance)

from sklearn.metrics import confusion_matrix, classification_report,ConfusionMatrixDisplay
import seaborn as sns
import matplotlib.pyplot as plt


cm = confusion_matrix(y_test, pred_dt)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)

disp.plot(cmap='Blues')
plt.title("Decision Tree")
plt.show()
print("\nClassification Report for Decision Tree")
print(classification_report(y_test, pred_dt,digits=4))





from sklearn.tree import export_text

tree_rules = export_text(DT, feature_names=list(feature_names), max_depth=3)
print(tree_rules)

n_nodes = DT.tree_.node_count
depth = DT.get_depth()

print(f"Число узлов в дереве: {n_nodes}")
print(f"Максимальная глубина дерева: {depth}")

from sklearn.ensemble import RandomForestClassifier

RF = RandomForestClassifier(random_state=0)
RF.fit(xv_train, y_train)
pred_dt = RF.predict(xv_test)
RF.score(xv_test, y_test)

RF.estimators_[0].tree_.max_depth

import numpy as np
import pandas as pd

importances = RF.feature_importances_
words = vectorizer.get_feature_names_out()
word_importances = pd.DataFrame({'word': words, 'importance': importances})
top20 = word_importances.sort_values(by='importance', ascending=False).head(20)

print(top20)

from sklearn.metrics import confusion_matrix, classification_report,ConfusionMatrixDisplay
import seaborn as sns
import matplotlib.pyplot as plt


cm = confusion_matrix(y_test, pred_dt)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)

disp.plot(cmap='Blues')
plt.title("Random Forest")
plt.show()
print("\nClassification Report for Random Forest")
print(classification_report(y_test, pred_dt,digits=4))

from sklearn.ensemble import GradientBoostingClassifier

R = GradientBoostingClassifier(learning_rate=0.8, n_estimators=300)
R.fit(xv_train, y_train)
pred_dt = R.predict(xv_test)
R.score(xv_test, y_test)
#

from sklearn.ensemble import GradientBoostingClassifier

R = GradientBoostingClassifier(learning_rate=0.8, n_estimators=500)
R.fit(xv_train, y_train)
pred_dt = R.predict(xv_test)
R.score(xv_test, y_test)
#

from sklearn.ensemble import GradientBoostingClassifier

R = GradientBoostingClassifier(learning_rate=0.8, n_estimators=1000)
R.fit(xv_train, y_train)
pred_dt = R.predict(xv_test)
R.score(xv_test, y_test)

from sklearn.metrics import confusion_matrix, classification_report,ConfusionMatrixDisplay
import seaborn as sns
import matplotlib.pyplot as plt
cm = confusion_matrix(y_test, pred_dt)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)

disp.plot(cmap='Blues')
plt.title("Gradient Boosting")
plt.show()
print("\nClassification Report for Gradient Boosting")
print(classification_report(y_test, pred_dt,digits=4))

import numpy as np

feature_names = vectorizer.get_feature_names_out()
importances = R.feature_importances_

indices = np.argsort(importances)[::-1][:20]

for i in indices:
    print(f"{feature_names[i]}: {importances[i]:.4f}")

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import TextVectorization, Embedding, Flatten, Dense
from tensorflow.keras.models import Sequential

vectorizer = TextVectorization(
    max_tokens=100000,
    output_sequence_length=64,
    standardize='lower_and_strip_punctuation'
)
vectorizer.adapt(x_train.tolist())
vectorized_train_text = vectorizer(x_train.tolist())
vectorized_test_text = vectorizer(x_test.tolist())
vectorized_val_text = vectorizer(x_val.tolist())
model = Sequential([
    Embedding(input_dim=100000, output_dim=256, input_length=64),
    Dropout(0.3),
    Flatten(),
    Dense(1, activation='sigmoid')
])


model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


model.fit(vectorized_train_text, y_train, epochs=10, batch_size=64, validation_data=(vectorized_test_text, y_test))
loss, accuracy = model.evaluate(vectorized_val_text, y_test)
print(f"Accuracy: {accuracy:.4f}")



from sklearn.metrics import classification_report

y_pred_probs = model.predict(vectorized_val_text)
y_pred = (y_pred_probs > 0.5).astype(int)

print(classification_report(y_val, y_pred, digits=4))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

cm = confusion_matrix(y_val, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Fake', 'Real'])
disp.plot(cmap='Blues')
plt.title(" Confusion Matrix")
plt.show()

model = Sequential([
    Embedding(input_dim=100000, output_dim=256, input_length=64),
    Dropout(0.3),
    LSTM(128),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 6. Обучаем модель
model.fit(vectorized_train_text, y_train, epochs=12, batch_size=64, validation_data=(vectorized_test_text, y_test))

loss, accuracy = model.evaluate(vectorized_val_text, y_val)
print(f"Accuracy: {accuracy:.4f}")

import tensorflow as tf
from tensorflow.keras.layers import (
    Input, Embedding, MultiHeadAttention, LayerNormalization,
    Dense, Dropout, GlobalAveragePooling1D
)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.regularizers import l2

class PositionalEmbedding(tf.keras.layers.Layer):
    def __init__(self, seq_length, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.seq_length = seq_length
        self.embed_dim = embed_dim
        self.pos_emb = tf.keras.layers.Embedding(
            input_dim=seq_length, output_dim=embed_dim
        )

    def call(self, x):
        positions = tf.range(start=0, limit=self.seq_length, delta=1)
        positions = tf.reshape(positions, (1, -1))
        pos_embeddings = self.pos_emb(positions)
        return x + pos_embeddings

    def get_config(self):
        config = super().get_config()
        config.update({
            "seq_length": self.seq_length,
            "embed_dim": self.embed_dim
        })
        return config

def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):
    x = MultiHeadAttention(
        num_heads=num_heads, key_dim=head_size
    )(inputs, inputs)
    x = Dropout(dropout)(x)
    x = LayerNormalization(epsilon=1e-6)(inputs + x)
    y = Dense(ff_dim, activation="gelu")(x)
    y = Dense(inputs.shape[-1])(y)
    y = Dropout(dropout)(y)
    return LayerNormalization(epsilon=1e-6)(x + y)

def build_transformer_model(
    max_tokens=100000,
    seq_length=64,
    embed_dim=128,
    num_heads=4,
    ff_dim=256,
    num_layers=4,
    dropout=0.1,
    l2_reg=1e-4
):
    inputs = Input(shape=(seq_length,))
    x = Embedding(input_dim=max_tokens, output_dim=embed_dim)(inputs)
    x = PositionalEmbedding(seq_length, embed_dim)(x)
    for _ in range(num_layers):
        x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout)
    x = GlobalAveragePooling1D()(x)
    outputs = Dense(
        1,
        activation="sigmoid",
        kernel_regularizer=l2(l2_reg)
    )(x)

    return Model(inputs=inputs, outputs=outputs)
model = build_transformer_model(
    max_tokens=100000,
    seq_length=64,
    embed_dim=128,
    num_heads=4,
    ff_dim=512,
    num_layers=4,
    dropout=0.1
)
total_steps = len(x_train) // 32 * 5  # 5 эпох
lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate=5e-5,
    end_learning_rate=1e-6,
    decay_steps=total_steps
)
optimizer = AdamW(learning_rate=lr_schedule, weight_decay=1e-4)

model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    vectorized_train_text, y_train,
    epochs=5,
    batch_size=32,
    validation_data=(vectorized_test_text, y_test)
)

loss, accuracy = model.evaluate(vectorized_val_text, y_val)
print(f"Accuracy: {accuracy:.4f}")

import tensorflow as tf
import numpy as np
from tqdm import tqdm
from transformers import TFBertModel, BertTokenizer
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling1D, MultiHeadAttention, LayerNormalization
from tensorflow.keras.optimizers import Adam

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def tokenize_and_save(texts, labels, tokenizer, max_len, save_path="tokenized_data.npz"):
    input_ids = []
    attention_masks = []

    for text in tqdm(texts, desc="Tokenizing"):
        tokens = tokenizer(
            text,
            padding='max_length',
            truncation=True,
            max_length=max_len,
            return_tensors='tf'
        )
        input_ids.append(tokens['input_ids'][0])
        attention_masks.append(tokens['attention_mask'][0])

    input_ids = tf.stack(input_ids)
    attention_masks = tf.stack(attention_masks)


    np.savez_compressed(
        save_path,
        input_ids=input_ids.numpy(),
        attention_mask=attention_masks.numpy(),
        labels=np.array(labels)
    )

    print(f"Сохранено в файл: {save_path}")
    return input_ids, attention_masks, labels

max_len = 64
train_encodings, _, y_train = tokenize_and_save(x_train.tolist(), y_train, tokenizer, max_len, save_path="train_tokenized.npz")
test_encodings, _, y_test = tokenize_and_save(x_test.tolist(), y_test, tokenizer, max_len, save_path="test_tokenized.npz")
test_encodings, _, y_test = tokenize_and_save(x_val.tolist(), y_val, tokenizer, max_len, save_path="val_tokenized.npz")

from tensorflow.keras.layers import Layer
from transformers import TFBertModel
class BertEmbedding(Layer):
    def __init__(self, model_name="bert-base-uncased", **kwargs):
        super().__init__(**kwargs)
        self.bert = TFBertModel.from_pretrained(model_name)
        self.bert.trainable = False
        for layer in self.bert.bert.encoder.layer[-3:]:
            layer.trainable = True

    def call(self, inputs):
        return self.bert(inputs).last_hidden_state
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):
    x = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)
    x = Dropout(dropout)(x)
    x = LayerNormalization(epsilon=1e-6)(inputs + x)

    y = Dense(ff_dim, activation="gelu")(x)
    y = Dense(inputs.shape[-1])(y)
    y = Dropout(dropout)(y)
    return LayerNormalization(epsilon=1e-6)(x + y)

from tensorflow.keras.optimizers import AdamW

max_len = 64
def load_tokenized(load_path="tokenized_data.npz"):
    data = np.load(load_path)
    input_ids = tf.convert_to_tensor(data['input_ids'])
    attention_mask = tf.convert_to_tensor(data['attention_mask'])
    labels = data['labels']
    print(f"📂 Загружено из файла: {load_path}")
    return input_ids, attention_mask, labels

train_input_ids, train_attention_mask, y_train = load_tokenized("train_tokenized.npz")
test_input_ids, test_attention_mask, y_test = load_tokenized("test_tokenized.npz")
val_input_ids, val_attention_mask, y_val = load_tokenized("val_tokenized.npz")

def build_model(max_len, num_heads=4, ff_dim=256, num_layers=2, dropout=0.1):
    input_ids = Input(shape=(max_len,), dtype=tf.int32, name="input_ids")
    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name="attention_mask")
    x = BertEmbedding()({"input_ids": input_ids, "attention_mask": attention_mask})

    for _ in range(num_layers):
        x = transformer_encoder(x, head_size=64, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout)

    x = GlobalAveragePooling1D()(x)
    x = Dropout(dropout)(x)
    output = Dense(1, activation="sigmoid")(x)

    model = Model(inputs=[input_ids, attention_mask], outputs=output)
    return model

model = build_model(max_len=max_len)
total_steps = len(x_train) // 32 * 5  # 5 эпох
lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate=5e-5,
    end_learning_rate=1e-6,
    decay_steps=total_steps
)
optimizer = AdamW(learning_rate=lr_schedule, weight_decay=1e-4)
model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.fit(
    x={
        'input_ids': train_input_ids,
        'attention_mask': train_attention_mask
    },
    y=y_train,
    validation_data=(
        {
            'input_ids': test_input_ids,
            'attention_mask': test_attention_mask
        },
        y_test
    ),
    epochs=5,
    batch_size=32
)