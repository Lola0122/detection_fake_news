# -*- coding: utf-8 -*-
"""–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15ftvl3tUVzgjLuctZyXBw8zJXfJwV8Tt
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
import re
import string

from google.colab import drive
drive.mount('/content/drive')

df_fake = pd.read_csv("/content/drive/My Drive/Colab Notebooks/Fake.csv")
df_true = pd.read_csv("/content/drive/My Drive/Colab Notebooks/True.csv")
df_fake["label"] = 0
df_true["label"] = 1

df = pd.concat([df_fake, df_true], axis =0 )

df = df.drop_duplicates(subset=['title'])
df = df.drop(["subject","date","text"], axis = 1)
print(df['label'].value_counts())

df = df.sample(frac=1, random_state=42).reset_index(drop=True)
df

def count_words(text):
    words = re.findall(r'\b\w+\b', text)
    return len(words)
df['len']=df["title"].apply(count_words)
df.describe()

df=df.drop(df[df['len'] == 0].index)
df=df.reset_index(drop=True)
df



print( (df['len'] < 46).sum()/df.shape[0])

import pandas as pd
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))

# –ö–æ–ª-–≤–æ –±–∏–Ω–æ–≤ –ø–æ 100 —Å–∏–º–≤–æ–ª–æ–≤
max_len = df['len'].max()
bins = range(0, int(max_len) + 10, 10)  # —à–∞–≥ 100

# –ü–æ—Å—Ç—Ä–æ–∏–º –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É –∏ –ø–æ–ª—É—á–∏–º –∑–Ω–∞—á–µ–Ω–∏—è
n, bins, patches = plt.hist(
    df['len'],

    edgecolor='black',
    color='purple',
    alpha=0.7,
    weights=[100 / len(df)] * len(df)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø—Ä–æ—Ü–µ–Ω—Ç—ã
)


plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤')
plt.ylabel('–ü—Ä–æ—Ü–µ–Ω—Ç —Ç–µ–∫—Å—Ç–æ–≤ (%)')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.xticks(bins, rotation=45)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞
max_len = df['len'].max()
bins = range(0, int(max_len) + 2, 5)  # —à–∞–≥ 1, —á—Ç–æ–±—ã —Å—á–∏—Ç–∞—Ç—å –∫–∞–∂–¥—É—é –¥–ª–∏–Ω—É —Å–ª–æ–≤–∞ –æ—Ç–¥–µ–ª—å–Ω–æ

# –°—Ç—Ä–æ–∏–º –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É
n, bins, patches = plt.hist(
    df['len'],
    bins=bins,
    edgecolor='black',
    color='purple',
    alpha=0.7
)

plt.xlabel('–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞(–≤ —Å–ª–æ–≤–∞—Ö)')
plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤')
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –¥–ª–∏–Ω–µ')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.xticks(bins, rotation=45)
plt.tight_layout()
plt.show()

def wordopt(text):
    text = text.lower()
    text = re.sub('watch:', '', text)
    text = re.sub('breaking:', '', text)
    text = re.sub('just in:', '', text)
    text = re.sub('report:', '', text)
    text = re.sub('breaking news:', '', text)
    text = re.sub('flashback:', '', text)
    text = re.sub('busted:', '', text)
    text = re.sub('bombshell:', '', text)
    text = re.sub('sunday screening:', '', text)
    text = re.sub('update:', '', text)
    text = re.sub('revealed:', '', text)
    text = re.sub('not kidding:', '', text)

    text = re.sub('video:', '', text)
    text = re.sub('breaking bombshell:', '', text)
    text = re.sub('factbox:', '', text)
    text = re.sub('exclusive:', '', text)
    text = re.sub('highlights:', '', text)
    text = re.sub('timeline:', '', text)
    text = re.sub('instant view:', '', text)



    text=re.sub(r'\S*/\S*', '', text)
    text = re.sub('\(.*?\)', '', text)
    text = re.sub(r'^.*?watch:', '', text)
    text=re.sub(r'^.*?report:', '', text)
    text=re.sub(r'^.*?breaking:', '', text)
    text = re.sub('\[.*?\]', '', text)
    #text = re.sub("\\W"," ",text)
    text = re.sub('https?://\S+|www\.\S+', '', text)  #????
    text = re.sub('<.*?>+', '', text)
    #text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    return text
df['title'] = df['title'].apply(wordopt)



import pandas as pd

# –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —É —Ç–µ–±—è –µ—Å—Ç—å DataFrame `df` —Å –∫–æ–ª–æ–Ω–∫–æ–π 'text'
mask = df['title'].str.lower().str.contains('hillary', case=False, na=False)

# –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç—Ä–æ–∫–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –ø–æ–¥—Å—Ç—Ä–æ–∫—É

matched_rows=df[df['label'] == 1]
matched_rows = matched_rows[mask]
print(f"üîç –ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–æ–∫: {len(matched_rows)}")
matched_rows

import pandas as pd
from collections import Counter
import re

# –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç—Ä–æ–∫–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –¥–≤–æ–µ—Ç–æ—á–∏–µ
mask = df['title'].str.contains(':', case=False, na=False)
titles_with_colon = df[mask]['title']

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–ª–æ–≤–∞ –ø–µ—Ä–µ–¥ –¥–≤–æ–µ—Ç–æ—á–∏–µ–º
def extract_before_colon(text):
    matches = re.findall(r'^([^:]+):', text)

    return matches

# –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
all_words_before_colon = []

for title in titles_with_colon:
    words = extract_before_colon(title)
    all_words_before_colon.extend(words)

# –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
counter = Counter(all_words_before_colon)
top_common = counter.most_common(20)

# –í—ã–≤–æ–¥–∏–º
print("üîç –¢–æ–ø 20 –ø–æ–¥—Å–ª–æ–≤ –ø–µ—Ä–µ–¥ –¥–≤–æ–µ—Ç–æ—á–∏–µ–º:")
for word, count in top_common:
    print(f"{word}: {count}")

import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter
import re

def extract_top_colon_starters(df, top_n=15):
    mask = df['title'].str.contains(':', case=False, na=False)
    titles_with_colon = df[mask]['title']

    def extract_before_colon(text):
        match = re.findall(r'^([^:]+):', text)
        return [m.strip() for m in match]  # üîß –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥

    all_words = []
    for title in titles_with_colon:
        words = extract_before_colon(title)
        all_words.extend(words)

    return Counter(all_words).most_common(top_n)

# –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ø —Å–ª–æ–≤–∞
top_fake = extract_top_colon_starters(df_fake, top_n=15)
top_true = extract_top_colon_starters(df_true, top_n=15)

# –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ –æ–¥–∏–Ω DataFrame
df_plot = pd.DataFrame({
    'word': [w for w, _ in top_fake + top_true],
    'count': [c for _, c in top_fake + top_true],
    'dataset': ['FAKE'] * len(top_fake) + ['REAL'] * len(top_true)
})

# –£–¥–∞–ª–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã —Å–ª–æ–≤ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
df_plot = df_plot.drop_duplicates(subset=['word', 'dataset'])

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(14, 7))
colors = {'FAKE': 'tomato', 'REAL': 'mediumseagreen'}

# –ü–æ—Å—Ç—Ä–æ–∏–º –±—Ä—É—Å–∫–∏ —Ä—è–¥–æ–º –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º
for label in df_plot['dataset'].unique():
    subset = df_plot[df_plot['dataset'] == label]
    plt.bar(subset['word'], subset['count'], label=label, alpha=0.8, color=colors[label])

plt.title('–¢–æ–ø 15 –ø–æ–¥—Å–ª–æ–≤ –ø–µ—Ä–µ–¥ ":" ', fontsize=14)
plt.xlabel('–ü–æ–¥—Å–ª–æ–≤–æ –ø–µ—Ä–µ–¥ ":"', fontsize=12)
plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞ –ø–æ—è–≤–ª–µ–Ω–∏—è', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

from sklearn.model_selection import train_test_split

# –°–Ω–∞—á–∞–ª–∞ train+val vs test
x_temp, x_test, y_temp, y_test = train_test_split(
    df['title'], df['label'],
    test_size=0.2,
    stratify=df['label'],
    random_state=42
)

# –ó–∞—Ç–µ–º train vs val
x_train, x_val, y_train, y_val = train_test_split(
    x_temp, y_temp,
    test_size=0.25,  # 0.25 * 0.8 = 0.2 –æ—Ç –æ–±—â–µ–≥–æ
    stratify=y_temp,
    random_state=42
)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤
print(f"Train: {len(x_train)}")
print(f"Validation: {len(x_val)}")
print(f"Test: {len(x_test)}")

print(y_val.value_counts())

print(df['label'].value_counts())

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —É —Ç–µ–±—è –µ—Å—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: y_train, y_val, y_test

# –°–æ–∑–¥–∞–¥–∏–º DataFrame –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
df_plot = pd.DataFrame({

    'train': pd.Series(y_train),
    'validation': pd.Series(y_val),
    'test': pd.Series(y_test),

})

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ "long-form" –¥–ª—è Seaborn
df_melted = df_plot.melt(var_name='Split', value_name='Label')

plt.figure(figsize=(9, 5))
sns.set_style("whitegrid")
sns.set_palette("pastel")

# –ü–æ—Å—Ç—Ä–æ–∏–º countplot
sns.countplot(data=df_melted, x='Split', hue='Label')

plt.ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π")
plt.xlabel("–í—ã–±–æ—Ä–∫–∞")
plt.legend(title="–ö–ª–∞—Å—Å", loc="upper right")
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
total = len(df['label'])
counts = df['label'].value_counts(normalize=True) * 100  # –í –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö

plt.figure(figsize=(6, 4))
sns.set_style("whitegrid")
palette = {'0': 'firebrick', '1': 'limegreen'}


ax = sns.countplot(x=df['label'], palette=palette)

# –ü–æ–¥–ø–∏—Å–∏ –≤ % –Ω–∞–¥ —Å—Ç–æ–ª–±–∏–∫–∞–º–∏
for p in ax.patches:
    percent = (p.get_height() / total) * 100
    ax.annotate(f'{percent:.1f}%', (p.get_x() + p.get_width() / 2., p.get_height() + 1),
                ha='center', va='bottom', fontsize=10)

plt.xticks([0, 1], ['Fake (0)', 'True (1)'])
plt.xlabel("–ö–ª–∞—Å—Å")
plt.ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π")
plt.title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ")
plt.tight_layout()
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords

vectorization = TfidfVectorizer()
xv_train =  TfidfVectorizer(stop_words=list(stopwords.words('english')), max_df=0.9, min_df=5)
xv_test = vectorization.transform(x_test)

import nltk
nltk.download('stopwords')

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import text
from nltk.corpus import stopwords
stop_words = text.ENGLISH_STOP_WORDS  # + —Å–≤–æ–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
vectorizer = TfidfVectorizer(stop_words=list(stopwords.words('english')), max_df=0.9, min_df=5)
xv_train = vectorizer.fit_transform(x_train)
xv_test = vectorizer.transform(x_test)

xv_train[0]

# –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test
from sklearn.model_selection import train_test_split


# –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –¢–û–õ–¨–ö–û –Ω–∞ X_train
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
scores = cross_val_score(model, xv_train, y_train, cv=5)

print("–°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ (–∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è):", scores.mean())

# –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ ‚Äî —É–∂–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç
model.fit(xv_train, y_train)
test_score = model.score(xv_test, y_test)
print("–û—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–º —Ç–µ—Å—Ç–µ:", test_score)

from sklearn.linear_model import LogisticRegression

LR = LogisticRegression()
LR.fit(xv_train,y_train)
pred_lr=LR.predict(xv_test)
LR.score(xv_test, y_test)

print(y_train.value_counts(normalize=True))
print(y_test.value_counts(normalize=True))

from sklearn.metrics import classification_report, f1_score

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —É–∂–µ –µ—Å—Ç—å:
# pred_lr = LR.predict(xv_test)

print("üîç Logistic Regression classification report:")
print(classification_report(y_test, pred_lr))

f1 = f1_score(y_test, pred_lr, average='weighted')  # –ò–ª–∏ 'macro', 'micro' ‚Äî –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∑–∞–¥–∞—á–∏
print(f"‚úÖ F1-score (weighted): {f1:.4f}")

from sklearn.metrics import classification_report, f1_score

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —É–∂–µ –µ—Å—Ç—å:
# pred_lr = LR.predict(xv_test)

print("üîç Logistic Regression classification report:")
print(classification_report(y_test, pred_lr))

f1 = f1_score(y_test, pred_lr, average='weighted')  # –ò–ª–∏ 'macro', 'micro' ‚Äî –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∑–∞–¥–∞—á–∏
print(f"‚úÖ F1-score (weighted): {f1:.4f}")

pattern = r'^([a-zA-Z]+):'

# –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
matches = [re.match(pattern, text) for text in x_train]
prefixes = [match.group(1).lower() for match in matches if match]  # –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –±–µ–∑ —É—á—ë—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞

# –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞
prefix_counts = Counter(prefixes)

print("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ —Å –¥–≤–æ–µ—Ç–æ—á–∏–µ–º –≤ –Ω–∞—á–∞–ª–µ:", len(prefixes))
print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞–º:", prefix_counts)

import numpy as np
print(np.bincount(y_train))
print(np.bincount(y_test))

from sklearn.metrics import classification_report, confusion_matrix

print(classification_report(y_test, pred_lr))
print(confusion_matrix(y_test, pred_lr))

from sklearn.metrics import classification_report, confusion_matrix

print(classification_report(y_test, pred_lr))
print(confusion_matrix(y_test, pred_lr))

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, pred_lr)

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["Fake", "Real"], yticklabels=[ "Fake","Real"])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

from sklearn.model_selection import cross_val_score
cv_scores = cross_val_score(LR, xv_train, y_train, cv=5, scoring='f1_weighted')
print(f"üìä Cross-validated F1-score: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")

feature_names = vectorization.get_feature_names_out()
coefficients = LR.coef_[0]

# –¢–æ–ø-20 —Å–ª–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∞ fake (1)
top_fake_idx = np.argsort(coefficients)[-20:]
# –¢–æ–ø-20 —Å–ª–æ–≤ –¥–ª—è real (0)
top_real_idx = np.argsort(coefficients)[:20]

print("\nüß™ Top 20 —Å–ª–æ–≤, —É–∫–∞–∑—ã–≤–∞—é—â–∏—Ö –Ω–∞ REAL:")
for idx in reversed(top_fake_idx):
    print(f"{feature_names[idx]}: {coefficients[idx]:.4f}")

print("\nüì∞ Top 20 —Å–ª–æ–≤, —É–∫–∞–∑—ã–≤–∞—é—â–∏—Ö –Ω–∞ FAKE:")
for idx in top_real_idx:
    print(f"{feature_names[idx]}: {coefficients[idx]:.4f}")

feature_names = vectorization.get_feature_names_out()
coefficients = LR.coef_[0]

# –¢–æ–ø-20 —Å–ª–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∞ fake (1)
top_fake_idx = np.argsort(coefficients)[-20:]
# –¢–æ–ø-20 —Å–ª–æ–≤ –¥–ª—è real (0)
top_real_idx = np.argsort(coefficients)[:20]

print("\nüß™ Top 20 —Å–ª–æ–≤, —É–∫–∞–∑—ã–≤–∞—é—â–∏—Ö –Ω–∞ REAL:")
for idx in reversed(top_fake_idx):
    print(f"{feature_names[idx]}: {coefficients[idx]:.4f}")

print("\nüì∞ Top 20 —Å–ª–æ–≤, —É–∫–∞–∑—ã–≤–∞—é—â–∏—Ö –Ω–∞ FAKE:")
for idx in top_real_idx:
    print(f"{feature_names[idx]}: {coefficients[idx]:.4f}")

import numpy as np
import matplotlib.pyplot as plt

# –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã —Å–∞–º—ã—Ö "—Ä–µ–∞–ª—å–Ω—ã—Ö" –∏ "—Ñ–µ–π–∫–æ–≤—ã—Ö" –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
top_fake_idx = np.argsort(coefficients)[-20:]
top_real_idx = np.argsort(coefficients)[:20]

# –°–ª–æ–≤–∞ –∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
top_fake_words = [(feature_names[i], coefficients[i]) for i in top_fake_idx]
top_real_words = [(feature_names[i], coefficients[i]) for i in top_real_idx]

# –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞
words = [w for w, _ in reversed(top_fake_words)] + [w for w, _ in top_real_words]
values = [v for _, v in reversed(top_fake_words)] + [v for _, v in top_real_words]

colors = ['mediumseagreen'] * 20 + ['tomato'] * 20

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(12, 10))
plt.barh(words, values, color=colors)
plt.axvline(0, color='black', linewidth=0.8)
plt.xlabel('–í–µ—Å —Å–ª–æ–≤–∞')
plt.ylabel('–°–ª–æ–≤–æ')
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

feature_names = vectorizer.get_feature_names_out()
coefficients = LR.coef_[0]

# –¢–æ–ø-20 —Å–ª–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∞ fake (1)
top_fake_idx = np.argsort(coefficients)[-20:]
# –¢–æ–ø-20 —Å–ª–æ–≤ –¥–ª—è real (0)
top_real_idx = np.argsort(coefficients)[:20]

print("\nüß™ Top 20 —Å–ª–æ–≤, —É–∫–∞–∑—ã–≤–∞—é—â–∏—Ö –Ω–∞ REAL:")
for idx in reversed(top_fake_idx):
    print(f"{feature_names[idx]}: {coefficients[idx]:.4f}")

print("\nüì∞ Top 20 —Å–ª–æ–≤, —É–∫–∞–∑—ã–≤–∞—é—â–∏—Ö –Ω–∞ FAKE:")
for idx in top_real_idx:
    print(f"{feature_names[idx]}: {coefficients[idx]:.4f}")

import numpy as np
import matplotlib.pyplot as plt

# –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã —Å–∞–º—ã—Ö "—Ä–µ–∞–ª—å–Ω—ã—Ö" –∏ "—Ñ–µ–π–∫–æ–≤—ã—Ö" –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
top_fake_idx = np.argsort(coefficients)[-20:]
top_real_idx = np.argsort(coefficients)[:20]

# –°–ª–æ–≤–∞ –∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
top_fake_words = [(feature_names[i], coefficients[i]) for i in top_fake_idx]
top_real_words = [(feature_names[i], coefficients[i]) for i in top_real_idx]

# –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞
words = [w for w, _ in reversed(top_fake_words)] + [w for w, _ in top_real_words]
values = [v for _, v in reversed(top_fake_words)] + [v for _, v in top_real_words]

colors = ['mediumseagreen'] * 20 + ['tomato'] * 20

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(12, 10))
plt.barh(words, values, color=colors)
plt.axvline(0, color='black', linewidth=0.8)
plt.xlabel('–í–µ—Å –ø—Ä–∏–∑–Ω–∞–∫–∞ (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ª–æ–≥—Ä–µ–≥—Ä–µ—Å—Å–∏–∏)')
plt.ylabel('–ü—Ä–∏–∑–Ω–∞–∫ (—Å–ª–æ–≤–æ)')
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

from sklearn.ensemble import RandomForestClassifier

DT = RandomForestClassifier(random_state=0)
DT.fit(xv_train, y_train)
pred_dt = DT.predict(xv_test)
DT.score(xv_test, y_test)

print(classification_report(y_test, pred_dt))

import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split

# –ú–æ–¥–µ–ª–∏
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(n_estimators=100),
    "Gradient Boosting": GradientBoostingClassifier()
}

plt.figure(figsize=(10, 7))

for name, model in models.items():
    # –û–±—É—á–µ–Ω–∏–µ
    model.fit(xv_train, y_train)

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    y_probs = model.predict_proba(xv_test)[:, 1]  # –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∞ "1"

    # ROC –∫—Ä–∏–≤–∞—è
    fpr, tpr, _ = roc_curve(y_test, y_probs)
    roc_auc = auc(fpr, tpr)

    # –û—Ç—Ä–∏—Å–æ–≤–∫–∞
    plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.3f})")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥—Ä–∞—Ñ–∏–∫–∞
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # –±–∞–∑–æ–≤–∞—è –ª–∏–Ω–∏—è
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC-–∫—Ä–∏–≤—ã–µ –º–æ–¥–µ–ª–µ–π')
plt.legend(loc='lower right')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.metrics import RocCurveDisplay

# –ú–æ–¥–µ–ª–∏
models = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "Random Forest": RandomForestClassifier()
}

plt.figure(figsize=(10, 8))

for name, model in models.items():
    model.fit(xv_train, y_train)

    # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (–Ω–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤, –∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏ –∫ –∫–ª–∞—Å—Å—É 1)
    probs = model.predict_proba(xv_test)[:, 1]

    # ROC: –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ FPR –∏ TPR
    fpr, tpr, _ = roc_curve(y_test, probs)
    roc_auc = auc(fpr, tpr)

    # –ì—Ä–∞—Ñ–∏–∫
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

# –î–∏–∞–≥–æ–Ω–∞–ª—å —Å–ª—É—á–∞–π–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC-–∫—Ä–∏–≤—ã–µ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π')
plt.legend(loc='lower right')
plt.grid()
plt.show()

from sklearn.metrics import precision_recall_curve, average_precision_score

plt.figure(figsize=(10, 8))

for name, model in models.items():
    probs = model.predict_proba(xv_test)[:, 1]
    precision, recall, _ = precision_recall_curve(y_test, probs)
    avg_precision = average_precision_score(y_test, probs)

    plt.plot(recall, precision, label=f'{name} (AP = {avg_precision:.2f})')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve –¥–ª—è –º–æ–¥–µ–ª–µ–π')
plt.legend(loc='upper right')
plt.grid()
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

for name, model in models.items():
    preds = model.predict(xv_test)
    cm = confusion_matrix(y_test, preds)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)

    print(f"üîé Confusion Matrix –¥–ª—è {name}")
    disp.plot(cmap='Blues')
    plt.title(name)
    plt.show()

from sklearn.metrics import classification_report

for name, model in models.items():
    preds = model.predict(xv_test)

    print(f" Classification Report –¥–ª—è {name}")
    print(classification_report(y_test, preds, digits=4))

"""------------------------------------------------"""

vectorizer = TextVectorization(max_tokens=20000,
                               output_sequence_length=600,
                               output_mode='int')
vectorizer.adapt(x_train.values.tolist())

vectorized_train_text = vectorizer(x_train.values)
vectorized_test_text = vectorizer(x_test.values)

def count_words(text):
    words = re.findall(r'\b\w+\b', text)
    return len(words)
df['len']=df["title"].apply(count_words)
df.describe()
df=df.drop(df[df['len'] == 0].index)
df=df.reset_index(drop=True)
df

df.describe()

LR = LogisticRegression()
LR.fit(vectorized_train_text,y_train)
pred_lr=LR.predict(vectorized_test_text)
LR.score(vectorized_test_text, y_test)

import numpy as np
from sklearn.linear_model import LogisticRegression
import tensorflow as tf
from tensorflow.keras.layers import TextVectorization

# 1. –°–æ–∑–¥–∞–Ω–∏–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞
vectorizer = TextVectorization(
    max_tokens=100000,  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
    output_sequence_length=600,  # –£–≤–µ–ª–∏—á–µ–Ω–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
    output_mode='int',
    standardize='lower_and_strip_punctuation'
)

# 2. –û–±—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞
vectorizer.adapt(x_train.tolist())  # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –ø–µ—Ä–µ–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫

# 3. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –≤–µ–∫—Ç–æ—Ä—ã
vectorized_train_text = vectorizer(x_train.tolist())
vectorized_test_text = vectorizer(x_test.tolist())

# 4. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ numpy-–º–∞—Å—Å–∏–≤ –¥–ª—è sklearn
vectorized_train_text = np.array(vectorized_train_text)
vectorized_test_text = np.array(vectorized_test_text)

# 5. –û–±—É—á–µ–Ω–∏–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
LR = LogisticRegression(max_iter=500)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —á–∏—Å–ª–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è –ª—É—á—à–µ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
LR.fit(vectorized_train_text, y_train)

# 6. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
pred_lr = LR.predict(vectorized_test_text)
accuracy = LR.score(vectorized_test_text, y_test)

print(f"Accuracy: {accuracy:.4f}")

import numpy as np
from tensorflow.keras.layers import TextVectorization, Embedding
from sklearn.linear_model import LogisticRegression
import tensorflow as tf

# 1. –ù–∞—Å—Ç—Ä–æ–∏–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
vectorizer = TextVectorization(
    max_tokens=50000,  # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (100000 ‚Üí 50000)
    output_sequence_length=400,  # –£–º–µ–Ω—å—à–∞–µ–º –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (600 ‚Üí 400)
    standardize='lower_and_strip_punctuation'
)
vectorizer.adapt(x_train.tolist())

# 2. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–æ–≤ (—Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä)
vectorized_train_text = vectorizer(np.array(x_train))
vectorized_test_text = vectorizer(np.array(x_test))

# 3. –°–æ–∑–¥–∞–µ–º —Å–ª–æ–π Embedding (–±–µ–∑ –æ–±—É—á–µ–Ω–∏—è)
embedding_layer = Embedding(input_dim=50000, output_dim=64, input_length=400)

# 4. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ —á–∞—Å—Ç—è–º (—á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å –ø–∞–º—è—Ç—å)
def get_embeddings(data, batch_size=512):
    num_samples = data.shape[0]
    embeddings = []

    for i in range(0, num_samples, batch_size):
        batch = data[i:i+batch_size]  # –ë–µ—Ä–µ–º –Ω–µ–±–æ–ª—å—à–æ–π –∫—É—Å–æ–∫ –¥–∞–Ω–Ω—ã—Ö
        embedded_batch = embedding_layer(batch)  # –ü—Ä–∏–º–µ–Ω—è–µ–º Embedding
        averaged_batch = tf.reduce_mean(embedded_batch, axis=1)  # –£—Å—Ä–µ–¥–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        embeddings.append(averaged_batch.numpy())  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º

    return np.vstack(embeddings)  # –°–∫–ª–µ–∏–≤–∞–µ–º –≤—Å–µ —á–∞—Å—Ç–∏ –≤ –æ–¥–∏–Ω –º–∞—Å—Å–∏–≤

# 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–ø–æ —á–∞—Å—Ç—è–º, –±–µ–∑ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è –ø–∞–º—è—Ç–∏)
embedded_train_text = get_embeddings(vectorized_train_text, batch_size=512)
embedded_test_text = get_embeddings(vectorized_test_text, batch_size=512)

# 6. –û–±—É—á–∞–µ–º –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é
LR = LogisticRegression(max_iter=500)
LR.fit(embedded_train_text, y_train)

# 7. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
accuracy = LR.score(embedded_test_text, y_test)
print(f"Accuracy: {accuracy:.4f}")

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import TextVectorization, Embedding, Flatten, Dense
from tensorflow.keras.models import Sequential

# 1. –ù–∞—Å—Ç—Ä–æ–∏–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
vectorizer = TextVectorization(
    max_tokens=100000,
    output_sequence_length=64,
    standardize='lower_and_strip_punctuation'
)

# 2. –û–±—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
vectorizer.adapt(x_train.tolist())

# 3. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–æ–≤
vectorized_train_text = vectorizer(x_train.tolist())
vectorized_test_text = vectorizer(x_test.tolist())

# 4. –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å Embedding-—Å–ª–æ–µ–º
model = Sequential([
    Embedding(input_dim=100000, output_dim=256, input_length=64),  # –≠–º–±–µ–¥–¥–∏–Ω–≥
     Dropout(0.3),
    Flatten(),  # –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º —Ç–µ–Ω–∑–æ—Ä –≤ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
    Dense(1, activation='sigmoid')  # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
])

# 5. –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 6. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
model.fit(vectorized_train_text, y_train, epochs=10, batch_size=64, validation_data=(vectorized_test_text, y_test))

# 7. –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å
loss, accuracy = model.evaluate(vectorized_test_text, y_test)
print(f"Accuracy: {accuracy:.4f}")

#17 –∞–ø—Ä–µ–ª—è –ø—Ä–µ–∑–∞

vectorized_val_text = vectorizer(x_val.tolist())
loss, accuracy = model.evaluate(vectorized_val_text, y_val)
print(f"Accuracy: {accuracy:.4f}")

from sklearn.metrics import classification_report

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ –¥–æ 0/1
y_pred_probs = model.predict(vectorized_val_text)
y_pred = (y_pred_probs > 0.5).astype(int)

# –û—Ç—á–µ—Ç –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º
print(classification_report(y_val, y_pred, digits=4))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

cm = confusion_matrix(y_val, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Fake', 'Real'])
disp.plot(cmap='Blues')
plt.title(" Confusion Matrix")
plt.show()

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import TextVectorization, Embedding, Flatten, Dense, LSTM,Dropout
from tensorflow.keras.models import Sequential

# 1. –ù–∞—Å—Ç—Ä–æ–∏–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
vectorizer = TextVectorization(
    max_tokens=100000,
    output_sequence_length=64,
    standardize='lower_and_strip_punctuation'
)

# 2. –û–±—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
vectorizer.adapt(x_train.tolist())

# 3. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–æ–≤
vectorized_train_text = vectorizer(x_train.tolist())
vectorized_test_text = vectorizer(x_test.tolist())

# 4. –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å Embedding-—Å–ª–æ–µ–º
model1 = Sequential([
    Embedding(input_dim=100000, output_dim=256, input_length=64),  # –≠–º–±–µ–¥–¥–∏–Ω–≥
    Dropout(0.3),
    LSTM(128),  # –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–π —Å–ª–æ–π
    Dropout(0.3),
    Dense(1, activation='sigmoid')  # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
])

# 5. –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 6. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
model1.fit(vectorized_train_text, y_train, epochs=12, batch_size=64, validation_data=(vectorized_test_text, y_test))

# 7. –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å
vectorized_val_text = vectorizer(x_val.tolist())
loss, accuracy = model1.evaluate(vectorized_val_text, y_val)
print(f"Accuracy: {accuracy:.4f}")

model1.fit(vectorized_train_text, y_train, epochs=1, batch_size=64, validation_data=(vectorized_test_text, y_test))

loss, accuracy = model1.evaluate(vectorized_val_text, y_val)
print(f"Accuracy: {accuracy:.4f}")

from sklearn.metrics import classification_report

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ –¥–æ 0/1
y_pred_probs = model1.predict(vectorized_val_text)
y_pred = (y_pred_probs > 0.5).astype(int)

# –û—Ç—á–µ—Ç –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º
print(classification_report(y_val, y_pred, digits=4))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

cm = confusion_matrix(y_val, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Fake', 'Real'])
disp.plot(cmap='Blues')
plt.title(" Confusion Matrix")
plt.show()

import tensorflow as tf
from tensorflow.keras.layers import (
    Input, Embedding, MultiHeadAttention, LayerNormalization,
    Dense, Dropout, GlobalAveragePooling1D
)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.regularizers import l2

# 1. –ö–ª–∞—Å—Å –¥–ª—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
class PositionalEmbedding(tf.keras.layers.Layer):
    def __init__(self, seq_length, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.seq_length = seq_length
        self.embed_dim = embed_dim
        self.pos_emb = tf.keras.layers.Embedding(
            input_dim=seq_length, output_dim=embed_dim
        )

    def call(self, x):
        positions = tf.range(start=0, limit=self.seq_length, delta=1)
        positions = tf.reshape(positions, (1, -1))
        pos_embeddings = self.pos_emb(positions)
        return x + pos_embeddings

    def get_config(self):
        config = super().get_config()
        config.update({
            "seq_length": self.seq_length,
            "embed_dim": self.embed_dim
        })
        return config

# 2. Transformer Encoder Block
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):
    # Self-Attention
    x = MultiHeadAttention(
        num_heads=num_heads, key_dim=head_size
    )(inputs, inputs)
    x = Dropout(dropout)(x)
    x = LayerNormalization(epsilon=1e-6)(inputs + x)  # Residual connection

    # Feed Forward Network
    y = Dense(ff_dim, activation="gelu")(x)  # GELU –∫–∞–∫ –≤ BERT
    y = Dense(inputs.shape[-1])(y)
    y = Dropout(dropout)(y)
    return LayerNormalization(epsilon=1e-6)(x + y)

# 3. –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å
def build_transformer_model(
    max_tokens=100000,
    seq_length=600,
    embed_dim=128,
    num_heads=4,
    ff_dim=256,
    num_layers=4,
    dropout=0.1,
    l2_reg=1e-4
):
    inputs = Input(shape=(seq_length,))

    # Embedding + Positional
    x = Embedding(input_dim=max_tokens, output_dim=embed_dim)(inputs)
    x = PositionalEmbedding(seq_length, embed_dim)(x)

    # Transformer Blocks
    for _ in range(num_layers):
        x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout)

    # Classifier
    x = GlobalAveragePooling1D()(x)
    outputs = Dense(
        1,
        activation="sigmoid",
        kernel_regularizer=l2(l2_reg)
    )(x)

    return Model(inputs=inputs, outputs=outputs)

# 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
model = build_transformer_model(
    max_tokens=100000,
    seq_length=64,
    embed_dim=128,
    num_heads=4,
    ff_dim=512,
    num_layers=4,
    dropout=0.1
)

# 5. AdamW —Å –ª–∏–Ω–µ–π–Ω—ã–º –∑–∞—Ç—É—Ö–∞–Ω–∏–µ–º LR
total_steps = len(x_train) // 32 * 5  # 5 —ç–ø–æ—Ö
lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate=5e-5,
    end_learning_rate=1e-6,
    decay_steps=total_steps
)
optimizer = AdamW(learning_rate=lr_schedule, weight_decay=1e-4)

# 6. –ö–æ–º–ø–∏–ª—è—Ü–∏—è
model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# 7. –û–±—É—á–µ–Ω–∏–µ
history = model.fit(
    vectorized_train_text, y_train,
    epochs=5,
    batch_size=32,
    validation_data=(vectorized_test_text, y_test)
)

# 8. –û—Ü–µ–Ω–∫–∞
loss, accuracy = model.evaluate(vectorized_val_text, y_val)
print(f"Accuracy: {accuracy:.4f}")

from sklearn.metrics import classification_report

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ –¥–æ 0/1
y_pred_probs = model.predict(vectorized_val_text)
y_pred = (y_pred_probs > 0.5).astype(int)

# –û—Ç—á–µ—Ç –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º
print(classification_report(y_val, y_pred, digits=4))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

cm = confusion_matrix(y_val, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Fake', 'Real'])
disp.plot(cmap='Blues')
plt.title(" Confusion Matrix")
plt.show()

history = model.fit(
    vectorized_train_text, y_train,
    epochs=5,
    batch_size=32,
    validation_data=(vectorized_test_text, y_test)
)

# 8. –û—Ü–µ–Ω–∫–∞
loss, accuracy = model.evaluate(vectorized_val_text, y_val)
print(f"Accuracy: {accuracy:.4f}")

from sklearn.metrics import classification_report

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ –¥–æ 0/1
y_pred_probs = model.predict(vectorized_val_text)
y_pred = (y_pred_probs > 0.5).astype(int)

# –û—Ç—á–µ—Ç –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º
print(classification_report(y_val, y_pred, digits=4))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

cm = confusion_matrix(y_val, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Fake', 'Real'])
disp.plot(cmap='Blues')
plt.title(" Confusion Matrix")
plt.show()

history = model.fit(
    vectorized_train_text, y_train,
    epochs=2,
    batch_size=32,
    validation_data=(vectorized_test_text, y_test)
)

# 8. –û—Ü–µ–Ω–∫–∞
loss, accuracy = model.evaluate(vectorized_val_text, y_val)
print(f"Accuracy: {accuracy:.4f}")
from sklearn.metrics import classification_report

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ –¥–æ 0/1
y_pred_probs = model.predict(vectorized_val_text)
y_pred = (y_pred_probs > 0.5).astype(int)

# –û—Ç—á–µ—Ç –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º
print(classification_report(y_val, y_pred, digits=4))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

cm = confusion_matrix(y_val, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Fake', 'Real'])
disp.plot(cmap='Blues')
plt.title(" Confusion Matrix")
plt.show()

"""—Å—Ç–∞—Ä–æ–µ"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import TextVectorization, Embedding, Dense, Input, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D, Dropout
from tensorflow.keras.models import Model

# 1. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä (–∫–∞–∫ —É –≤–∞—Å)
vectorizer = TextVectorization(
    max_tokens=100000,
    output_sequence_length=600,
    standardize='lower_and_strip_punctuation'
)
vectorizer.adapt(x_train.tolist())

# 2. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
vectorized_train_text = vectorizer(x_train.tolist())
vectorized_test_text = vectorizer(x_test.tolist())

# 3. –°–æ–∑–¥–∞—ë–º Transformer-–º–æ–¥–µ–ª—å
def build_transformer_model():
    inputs = Input(shape=(600,), dtype=tf.int32)  # input_length=600

    # –≠–º–±–µ–¥–¥–∏–Ω–≥ + –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ (—É–ø—Ä–æ—â—ë–Ω–Ω–æ)
    x = Embedding(input_dim=100000, output_dim=128)(inputs)

    # Transformer Block
    x = MultiHeadAttention(num_heads=4, key_dim=128)(x, x)  # Self-Attention
    x = LayerNormalization(epsilon=1e-6)(x)
    x = Dropout(0.1)(x)

    # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –æ—Å–∏
    x = GlobalAveragePooling1D()(x)

    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
    outputs = Dense(1, activation='sigmoid')(x)

    return Model(inputs=inputs, outputs=outputs)

model = build_transformer_model()

# 4. –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 5. –û–±—É—á–∞–µ–º
history = model.fit(
    vectorized_train_text, y_train,
    epochs=5,
    batch_size=32,
    validation_data=(vectorized_test_text, y_test)
)

# 6. –û—Ü–µ–Ω–∫–∞
loss, accuracy = model.evaluate(vectorized_test_text, y_test)
print(f"Accuracy: {accuracy:.4f}")

import tensorflow as tf
from tensorflow.keras.layers import (
    Input, Embedding, MultiHeadAttention, LayerNormalization,
    Dense, Dropout, GlobalAveragePooling1D
)
from tensorflow.keras.models import Model

def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):
    # Self-Attention
    x = MultiHeadAttention(
        num_heads=num_heads, key_dim=head_size
    )(inputs, inputs)
    x = Dropout(dropout)(x)
    x = LayerNormalization(epsilon=1e-6)(inputs + x)  # Residual connection

    # Feed Forward Network
    y = Dense(ff_dim, activation="relu")(x)
    y = Dense(inputs.shape[-1])(y)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
    y = Dropout(dropout)(y)
    return LayerNormalization(epsilon=1e-6)(x + y)  # –ï—â—ë residual

def build_transformer_model(
    max_tokens=100000,
    seq_length=600,
    embed_dim=128,
    num_heads=4,
    ff_dim=256,
    num_layers=4,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ Transformer-–±–ª–æ–∫–æ–≤
    dropout=0.1
):
    inputs = Input(shape=(seq_length,))

    # Embedding
    x = Embedding(input_dim=max_tokens, output_dim=embed_dim)(inputs)

    # –ù–µ—Å–∫–æ–ª—å–∫–æ Transformer-–±–ª–æ–∫–æ–≤
    for _ in range(num_layers):
        x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout)

    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
    x = GlobalAveragePooling1D()(x)
    outputs = Dense(1, activation="sigmoid")(x)

    return Model(inputs=inputs, outputs=outputs)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ (–∫–∞–∫ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –∫–æ–¥–µ)
model = build_transformer_model(
    max_tokens=100000,
    num_layers=4,  # 4 –±–ª–æ–∫–∞ –∫–∞–∫ –≤ BERT-base
    ff_dim=512     # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä FFN
)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(vectorized_train_text, y_train, epochs=5, batch_size=32,
          validation_data=(vectorized_test_text, y_test))

loss, accuracy = model.evaluate(vectorized_test_text, y_test)
print(f"Accuracy: {accuracy:.4f}")

import tensorflow as tf
from tensorflow.keras.layers import (
    Input, Embedding, MultiHeadAttention, LayerNormalization,
    Dense, Dropout, GlobalAveragePooling1D
)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.regularizers import l2

# 1. –ö–ª–∞—Å—Å –¥–ª—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
class PositionalEmbedding(tf.keras.layers.Layer):
    def __init__(self, seq_length, embed_dim):
        super().__init__()
        self.seq_length = seq_length
        self.embed_dim = embed_dim
        self.pos_emb = tf.keras.layers.Embedding(
            input_dim=seq_length, output_dim=embed_dim
        )

    def call(self, x):
        positions = tf.range(start=0, limit=self.seq_length, delta=1)
        positions = tf.reshape(positions, (1, -1))
        pos_embeddings = self.pos_emb(positions)
        return x + pos_embeddings

# 2. Transformer Encoder Block
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):
    # Self-Attention
    x = MultiHeadAttention(
        num_heads=num_heads, key_dim=head_size
    )(inputs, inputs)
    x = Dropout(dropout)(x)
    x = LayerNormalization(epsilon=1e-6)(inputs + x)  # Residual connection

    # Feed Forward Network
    y = Dense(ff_dim, activation="gelu")(x)  # GELU –∫–∞–∫ –≤ BERT
    y = Dense(inputs.shape[-1])(y)
    y = Dropout(dropout)(y)
    return LayerNormalization(epsilon=1e-6)(x + y)

# 3. –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å
def build_transformer_model(
    max_tokens=100000,
    seq_length=600,
    embed_dim=128,
    num_heads=4,
    ff_dim=256,
    num_layers=4,
    dropout=0.1,
    l2_reg=1e-4
):
    inputs = Input(shape=(seq_length,))

    # Embedding + Positional
    x = Embedding(input_dim=max_tokens, output_dim=embed_dim)(inputs)
    x = PositionalEmbedding(seq_length, embed_dim)(x)

    # Transformer Blocks
    for _ in range(num_layers):
        x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout)

    # Classifier
    x = GlobalAveragePooling1D()(x)
    outputs = Dense(
        1,
        activation="sigmoid",
        kernel_regularizer=l2(l2_reg)
    )(x)

    return Model(inputs=inputs, outputs=outputs)

# 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
model = build_transformer_model(
    max_tokens=100000,
    seq_length=600,
    embed_dim=128,
    num_heads=4,
    ff_dim=512,
    num_layers=4,
    dropout=0.1
)

# 5. AdamW —Å –ª–∏–Ω–µ–π–Ω—ã–º –∑–∞—Ç—É—Ö–∞–Ω–∏–µ–º LR
total_steps = len(x_train) // 32 * 5  # 5 —ç–ø–æ—Ö
lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate=5e-5,
    end_learning_rate=1e-6,
    decay_steps=total_steps
)
optimizer = AdamW(learning_rate=lr_schedule, weight_decay=1e-4)

# 6. –ö–æ–º–ø–∏–ª—è—Ü–∏—è
model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# 7. –û–±—É—á–µ–Ω–∏–µ
history = model.fit(
    vectorized_train_text, y_train,
    epochs=5,
    batch_size=32,
    validation_data=(vectorized_test_text, y_test)
)

# 8. –û—Ü–µ–Ω–∫–∞
loss, accuracy = model.evaluate(vectorized_test_text, y_test)
print(f"Accuracy: {accuracy:.4f}")

# 2. –û–±—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
from tensorflow.keras.layers import TextVectorization
vectorizer = TextVectorization(max_tokens=20000,
                               output_sequence_length=600,
                               output_mode='int')
vectorizer.adapt(x_train.tolist())

# 3. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–æ–≤
vectorized_train_text = vectorizer(x_train.tolist())
vectorized_test_text = vectorizer(x_test.tolist())

import tensorflow as tf
from tensorflow.keras.layers import (
    Input, Embedding, MultiHeadAttention, LayerNormalization,
    Dense, Dropout, GlobalAveragePooling1D
)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.regularizers import l2

# 1. –ö–ª–∞—Å—Å –¥–ª—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
class PositionalEmbedding(tf.keras.layers.Layer):
    def __init__(self, seq_length, embed_dim):
        super().__init__()
        self.seq_length = seq_length
        self.embed_dim = embed_dim
        self.pos_emb = tf.keras.layers.Embedding(
            input_dim=seq_length, output_dim=embed_dim
        )

    def call(self, x):
        positions = tf.range(start=0, limit=self.seq_length, delta=1)
        positions = tf.reshape(positions, (1, -1))
        pos_embeddings = self.pos_emb(positions)
        return x + pos_embeddings

# 2. Transformer Encoder Block
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):
    # Self-Attention
    x = MultiHeadAttention(
        num_heads=num_heads, key_dim=head_size
    )(inputs, inputs)
    x = Dropout(dropout)(x)
    x = LayerNormalization(epsilon=1e-6)(inputs + x)  # Residual connection

    # Feed Forward Network
    y = Dense(ff_dim, activation="gelu")(x)  # GELU –∫–∞–∫ –≤ BERT
    y = Dense(inputs.shape[-1])(y)
    y = Dropout(dropout)(y)
    return LayerNormalization(epsilon=1e-6)(x + y)

# 3. –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å
def build_transformer_model(
    max_tokens=100000,
    seq_length=600,
    embed_dim=128,
    num_heads=4,
    ff_dim=256,
    num_layers=4,
    dropout=0.1,
    l2_reg=1e-4
):
    inputs = Input(shape=(seq_length,))

    # Embedding + Positional
    x = Embedding(input_dim=max_tokens, output_dim=embed_dim)(inputs)
    x = PositionalEmbedding(seq_length, embed_dim)(x)

    # Transformer Blocks
    for _ in range(num_layers):
        x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout)

    # Classifier
    x = GlobalAveragePooling1D()(x)
    outputs = Dense(
        1,
        activation="sigmoid",
        kernel_regularizer=l2(l2_reg)
    )(x)

    return Model(inputs=inputs, outputs=outputs)

# 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
model = build_transformer_model(
    max_tokens=100000,
    seq_length=600,
    embed_dim=128,
    num_heads=8,
    ff_dim=512,
    num_layers=4,
    dropout=0.1
)

# 5. AdamW —Å –ª–∏–Ω–µ–π–Ω—ã–º –∑–∞—Ç—É—Ö–∞–Ω–∏–µ–º LR
total_steps = len(x_train) // 32 * 5  # 5 —ç–ø–æ—Ö
lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate=5e-5,
    end_learning_rate=1e-6,
    decay_steps=total_steps
)
optimizer = AdamW(learning_rate=lr_schedule, weight_decay=1e-4)

# 6. –ö–æ–º–ø–∏–ª—è—Ü–∏—è
model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# 7. –û–±—É—á–µ–Ω–∏–µ
history = model.fit(
    vectorized_train_text, y_train,
    epochs=5,
    batch_size=32,
    validation_data=(vectorized_test_text, y_test)
)

# 8. –û—Ü–µ–Ω–∫–∞
loss, accuracy = model.evaluate(vectorized_test_text, y_test)
print(f"Accuracy: {accuracy:.4f}")

import tensorflow as tf
from tensorflow.keras.layers import (
    Input, Embedding, MultiHeadAttention, LayerNormalization,
    Dense, Dropout, GlobalAveragePooling1D
)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.regularizers import l2

# 1. –ö–ª–∞—Å—Å –¥–ª—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
class PositionalEmbedding(tf.keras.layers.Layer):
    def __init__(self, seq_length, embed_dim):
        super().__init__()
        self.seq_length = seq_length
        self.embed_dim = embed_dim
        self.pos_emb = tf.keras.layers.Embedding(
            input_dim=seq_length, output_dim=embed_dim
        )

    def call(self, x):
        positions = tf.range(start=0, limit=self.seq_length, delta=1)
        positions = tf.reshape(positions, (1, -1))
        pos_embeddings = self.pos_emb(positions)
        return x + pos_embeddings

# 2. Transformer Encoder Block
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):
    # Self-Attention
    x = MultiHeadAttention(
        num_heads=num_heads, key_dim=head_size
    )(inputs, inputs)
    x = Dropout(dropout)(x)
    x = LayerNormalization(epsilon=1e-6)(inputs + x)  # Residual connection

    # Feed Forward Network
    y = Dense(ff_dim, activation="gelu")(x)  # GELU –∫–∞–∫ –≤ BERT
    y = Dense(inputs.shape[-1])(y)
    y = Dropout(dropout)(y)
    return LayerNormalization(epsilon=1e-6)(x + y)

# 3. –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å
def build_transformer_model(
    max_tokens=100000,
    seq_length=600,
    embed_dim=128,
    num_heads=8,
    ff_dim=256,
    num_layers=4,
    dropout=0.1,
    l2_reg=1e-4
):
    inputs = Input(shape=(seq_length,))

    # Embedding + Positional
    x = Embedding(input_dim=max_tokens, output_dim=embed_dim)(inputs)
    x = PositionalEmbedding(seq_length, embed_dim)(x)

    # Transformer Blocks
    for _ in range(num_layers):
        x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout)

    # Classifier
    x = GlobalAveragePooling1D()(x)
    outputs = Dense(
        1,
        activation="sigmoid",
        kernel_regularizer=l2(l2_reg)
    )(x)

    return Model(inputs=inputs, outputs=outputs)

# 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
model = build_transformer_model(
    max_tokens=100000,
    seq_length=600,
    embed_dim=256,
    num_heads=4,
    ff_dim=512,
    num_layers=4,
    dropout=0.1
)

# 5. AdamW —Å –ª–∏–Ω–µ–π–Ω—ã–º –∑–∞—Ç—É—Ö–∞–Ω–∏–µ–º LR
total_steps = len(x_train) // 32 * 5  # 5 —ç–ø–æ—Ö
lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate=5e-5,
    end_learning_rate=1e-6,
    decay_steps=total_steps
)
optimizer = AdamW(learning_rate=lr_schedule, weight_decay=1e-4)

# 6. –ö–æ–º–ø–∏–ª—è—Ü–∏—è
model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# 7. –û–±—É—á–µ–Ω–∏–µ
history = model.fit(
    vectorized_train_text, y_train,
    epochs=5,
    batch_size=32,
    validation_data=(vectorized_test_text, y_test)
)

# 8. –û—Ü–µ–Ω–∫–∞
loss, accuracy = model.evaluate(vectorized_test_text, y_test)
print(f"Accuracy: {accuracy:.4f}")

import tensorflow as tf
from tensorflow.keras.layers import (
    Input, Embedding, MultiHeadAttention, LayerNormalization,
    Dense, Dropout, GlobalAveragePooling1D
)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.regularizers import l2

# 1. –ö–ª–∞—Å—Å –¥–ª—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
class PositionalEmbedding(tf.keras.layers.Layer):
    def __init__(self, seq_length, embed_dim):
        super().__init__()
        self.seq_length = seq_length
        self.embed_dim = embed_dim
        self.pos_emb = tf.keras.layers.Embedding(
            input_dim=seq_length, output_dim=embed_dim
        )

    def call(self, x):
        positions = tf.range(start=0, limit=self.seq_length, delta=1)
        positions = tf.reshape(positions, (1, -1))
        pos_embeddings = self.pos_emb(positions)
        return x + pos_embeddings

# 2. Transformer Encoder Block
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):
    # Self-Attention
    x = MultiHeadAttention(
        num_heads=num_heads, key_dim=head_size
    )(inputs, inputs)
    x = Dropout(dropout)(x)
    x = LayerNormalization(epsilon=1e-6)(inputs + x)  # Residual connection

    # Feed Forward Network
    y = Dense(ff_dim, activation="gelu")(x)  # GELU –∫–∞–∫ –≤ BERT
    y = Dense(inputs.shape[-1])(y)
    y = Dropout(dropout)(y)
    return LayerNormalization(epsilon=1e-6)(x + y)

# 3. –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å
def build_transformer_model(
    max_tokens=100000,
    seq_length=600,
    embed_dim=128,
    num_heads=4,
    ff_dim=256,
    num_layers=4,
    dropout=0.1,
    l2_reg=1e-4
):
    inputs = Input(shape=(seq_length,))

    # Embedding + Positional
    x = Embedding(input_dim=max_tokens, output_dim=embed_dim)(inputs)
    x = PositionalEmbedding(seq_length, embed_dim)(x)

    # Transformer Blocks
    for _ in range(num_layers):
        x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout)

    # Classifier
    x = GlobalAveragePooling1D()(x)
    outputs = Dense(
        1,
        activation="sigmoid",
        kernel_regularizer=l2(l2_reg)
    )(x)

    return Model(inputs=inputs, outputs=outputs)

# 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
model = build_transformer_model(
    max_tokens=100000,
    seq_length=600,
    embed_dim=512,
    num_heads=8,
    ff_dim=512,
    num_layers=4,
    dropout=0.1
)

# 5. AdamW —Å –ª–∏–Ω–µ–π–Ω—ã–º –∑–∞—Ç—É—Ö–∞–Ω–∏–µ–º LR
total_steps = len(x_train) // 32 * 5  # 5 —ç–ø–æ—Ö
lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate=5e-5,
    end_learning_rate=1e-6,
    decay_steps=total_steps
)
optimizer = AdamW(learning_rate=lr_schedule, weight_decay=1e-4)

# 6. –ö–æ–º–ø–∏–ª—è—Ü–∏—è
model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# 7. –û–±—É—á–µ–Ω–∏–µ
history = model.fit(
    vectorized_train_text, y_train,
    epochs=5,
    batch_size=32,
    validation_data=(vectorized_test_text, y_test)
)

# 8. –û—Ü–µ–Ω–∫–∞
loss, accuracy = model.evaluate(vectorized_test_text, y_test)
print(f"Accuracy: {accuracy:.4f}")

model.save_weights("transformer512.weights.h5")

from google.colab import drive
drive.mount('/content/drive')

a = pd.read_csv("/content/drive/My Drive/News_test_combined.csv")
b = pd.read_csv("/content/drive/My Drive/News_train_combined.csv")
df = pd.concat([a, b], axis =0 )
df

model.save_weights('/content/drive/My Drive/transformer512.weights.h5')

model = build_transformer_model(
    max_tokens=100000,
    seq_length=600,
    embed_dim=512,
    num_heads=8,
    ff_dim=512,
    num_layers=4,
    dropout=0.1
)
total_steps = len(x_train) // 32 * 5  # 5 —ç–ø–æ—Ö
lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate=5e-5,
    end_learning_rate=1e-6,
    decay_steps=total_steps
)
optimizer = AdamW(learning_rate=lr_schedule, weight_decay=1e-4)

# 6. –ö–æ–º–ø–∏–ª—è—Ü–∏—è
model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',
    metrics=['accuracy']
)
# –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –≤ –º–æ–¥–µ–ª—å
model.load_weights("transformer512.weights.h5")
loss, accuracy = model.evaluate(vectorized_test_text, y_test)
print(f"Accuracy: {accuracy:.4f}")

import tensorflow as tf
from tensorflow.keras.layers import (
    Input, Embedding, MultiHeadAttention, LayerNormalization,
    Dense, Dropout, GlobalAveragePooling1D
)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.regularizers import l2

# 1. –ö–ª–∞—Å—Å –¥–ª—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
class PositionalEmbedding(tf.keras.layers.Layer):
    def __init__(self, seq_length, embed_dim):
        super().__init__()
        self.seq_length = seq_length
        self.embed_dim = embed_dim
        self.pos_emb = tf.keras.layers.Embedding(
            input_dim=seq_length, output_dim=embed_dim
        )

    def call(self, x):
        positions = tf.range(start=0, limit=self.seq_length, delta=1)
        positions = tf.reshape(positions, (1, -1))
        pos_embeddings = self.pos_emb(positions)
        return x + pos_embeddings

# 2. Transformer Encoder Block
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):
    # Self-Attention
    x = MultiHeadAttention(
        num_heads=num_heads, key_dim=head_size
    )(inputs, inputs)
    x = Dropout(dropout)(x)
    x = LayerNormalization(epsilon=1e-6)(inputs + x)  # Residual connection

    # Feed Forward Network
    y = Dense(ff_dim, activation="gelu")(x)  # GELU –∫–∞–∫ –≤ BERT
    y = Dense(inputs.shape[-1])(y)
    y = Dropout(dropout)(y)
    return LayerNormalization(epsilon=1e-6)(x + y)

# 3. –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å
def build_transformer_model(
    max_tokens=100000,
    seq_length=600,
    embed_dim=128,
    num_heads=4,
    ff_dim=256,
    num_layers=4,
    dropout=0.1,
    l2_reg=1e-4
):
    inputs = Input(shape=(seq_length,))

    # Embedding + Positional
    x = Embedding(input_dim=max_tokens, output_dim=embed_dim)(inputs)
    x = PositionalEmbedding(seq_length, embed_dim)(x)

    # Transformer Blocks
    for _ in range(num_layers):
        x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout)

    # Classifier
    x = GlobalAveragePooling1D()(x)
    outputs = Dense(
        1,
        activation="sigmoid",
        kernel_regularizer=l2(l2_reg)
    )(x)

    return Model(inputs=inputs, outputs=outputs)

from tensorflow.keras.layers import TextVectorization
vectorizer = TextVectorization(
    max_tokens=100000,
    output_sequence_length=600,
    standardize='lower_and_strip_punctuation'
)
vectorizer.adapt(x_train.tolist())

# 2. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
vectorized_train_text = vectorizer(x_train.tolist())
vectorized_test_text = vectorizer(x_test.tolist())

vectorized_test_text

model = build_transformer_model(
    max_tokens=100000,
    seq_length=600,
    embed_dim=512,
    num_heads=8,
    ff_dim=512,
    num_layers=4,
    dropout=0.1
)
total_steps = len(x_train) // 32 * 5  # 5 —ç–ø–æ—Ö
lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate=5e-5,
    end_learning_rate=1e-6,
    decay_steps=total_steps
)
optimizer = AdamW(learning_rate=lr_schedule, weight_decay=1e-4)

# 6. –ö–æ–º–ø–∏–ª—è—Ü–∏—è
model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',
    metrics=['accuracy']
)
# –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –≤ –º–æ–¥–µ–ª—å
model.load_weights("/content/drive/My Drive/transformer512.weights.h5")
loss, accuracy = model.evaluate(vectorized_test_text, y_test)
print(f"Accuracy: {accuracy:.4f}")

tt["text"] = tt["text"].apply(wordopt)
tt

vect_tt = vectorizer(tt["text"].tolist())

# –í–∞—à–∞ —Å—Ç—Ä–æ–∫–∞ –¥–ª—è —Ç–µ—Å—Ç–∞ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤ –≤–∏–¥–µ —Å–ø–∏—Å–∫–∞ —Å—Ç—Ä–æ–∫!)
test_strings = [
# 1. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
vectorized_text = vectorizer(test_strings)  # vectorizer –æ–∂–∏–¥–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫

# 2. –°–æ–∑–¥–∞—ë–º "–∑–∞–≥–ª—É—à–∫—É" –¥–ª—è y_test (–µ—Å–ª–∏ –º–µ—Ç–∫–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã)
dummy_y =  [1]# –ù–∞–ø—Ä–∏–º–µ—Ä, –º–∞—Å—Å–∏–≤ –Ω—É–ª–µ–π —Ç–æ–π –∂–µ –¥–ª–∏–Ω—ã
dummy_y = np.reshape(dummy_y, (-1, 1))
# 4. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω–∞ accuracy)
loss, accuracy = model.evaluate(vect_tt, dummy_y)
print(f"Accuracy: {accuracy:.4f}")

tt=pd.read_csv("tt.csv")
tt

import pandas as pd

news_data = pd.DataFrame({
    'id': [1],

    'text': ['–°–µ–≥–æ–¥–Ω—è Apple –∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–ª–∞ —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä M2...'],

})

news_data.to_csv('news_pandas.csv', index=False, encoding='utf-8')

import torch
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from tqdm import tqdm  # Progress bar

# –ò—Å–ø–æ–ª—å–∑—É–µ–º DistilBERT (–æ–Ω –±—ã—Å—Ç—Ä–µ–µ RoBERTa)
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
model.eval()  # –†–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏ (—É—Å–∫–æ—Ä—è–µ—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å)

# PyTorch Dataset –¥–ª—è –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
class FakeNewsDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encodings = tokenizer(self.texts[idx], padding="max_length", truncation=True, max_length=256, return_tensors="pt")
        return {key: val.squeeze(0) for key, val in encodings.items()}, torch.tensor(self.labels[idx])

# DataLoader (–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —á–∞—Å—Ç—è–º–∏)
batch_size = 32
test_dataset = FakeNewsDataset(x_test.tolist(), y_test.tolist())
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# –§—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
def evaluate(model, dataloader):
    correct = 0
    total = 0

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Processing Batches", unit="batch"):
            inputs, labels = batch
            outputs = model(**inputs)  # –í–æ—Ç —Ç—É—Ç —Ç–æ—Ä–º–æ–∑–∏–ª–æ
            predictions = torch.argmax(outputs.logits, dim=1)
            correct += (predictions == labels).sum().item()
            total += labels.size(0)

    return correct / total

# –û—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏
accuracy = evaluate(model, test_loader)
print(f"Accuracy: {accuracy:.4f}")

"""### **–ü—ã—Ç–∞–µ–º—Å—è —É–±—Ä–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –Ω–æ–≤–æ—Å—Ç–µ–π!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!**"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
import re
import string

from google.colab import drive
drive.mount('/content/drive')

df_fake = pd.read_csv("/content/drive/My Drive/Colab Notebooks/Fake.csv")
df_true = pd.read_csv("/content/drive/My Drive/Colab Notebooks/True.csv")

df_fake

df_true

df_fake = df_fake.drop_duplicates(subset=['text'])
df_fake

df_true = df_true.drop_duplicates(subset=['text'])
df_true

df_fake["label"] = 0
df_true["label"] = 1

df = pd.concat([df_fake, df_true], axis =0 )
df = df.drop(["title", "subject","date"], axis = 1)
df

df = df.sample(frac = 1)

df

df=df.drop(df[df['text'] == " "].index)
df.reset_index(inplace = True)
df

df.reset_index(inplace = True)
df.drop(["index"], axis = 1, inplace = True)
df

df_X=df["text"]
df_Y=df["label"]

import spacy
from tqdm.notebook import tqdm
en = spacy.load('en_core_web_sm')
def lemmatize(text):
    a = en(text)
    return ' '.join([token.lemma_ for token in a if
                  not token.is_stop])
tqdm.pandas()
df_X = df_X.progress_apply(lemmatize)

df = pd.concat([df_X.reset_index(drop=True),
                       df_Y.reset_index(drop=True)], axis=1)

df.to_csv('/content/drive/My Drive/DATA_NEWS.csv', index=False)

df=pd.read_csv('/content/drive/My Drive/DATA_NEWS.csv')

def wordopt(text):
    text = text.lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub("\\W"," ",text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    return text
df["text"] = df["text"].apply(wordopt)

def count_words(text):
    words = re.findall(r'\b\w+\b', text)
    return len(words)
df['len']=df["text"].apply(count_words)

df=df.drop(df[df['len'] == 0].index)
df

df.reset_index(inplace = True)
df

x = df["text"]
y = df["label"]
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)

df.describe()

import pandas as pd

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
train_texts = set(x_train.dropna())
test_texts = set(x_test.dropna())

# –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
exact_matches = train_texts & test_texts

print(f"–ù–∞–π–¥–µ–Ω–æ {len(exact_matches)} —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π:")
for match in list(exact_matches)[:5]:  # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 5 –ø—Ä–∏–º–µ—Ä–æ–≤
    print(f"- {match[:100]}...")  # –û–±—Ä–µ–∑–∞–µ–º –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞

# 2. –û–±—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
from tensorflow.keras.layers import TextVectorization
vectorizer = TextVectorization(max_tokens=20000,
                               output_sequence_length=600,
                               output_mode='int')
vectorizer.adapt(x_train.tolist())

# 3. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–æ–≤
vectorized_train_text = vectorizer(x_train.tolist())
vectorized_test_text = vectorizer(x_test.tolist())

import tensorflow as tf
vectorizer_model = tf.keras.Sequential([vectorizer])
vectorizer_model.save("NEWtext_vectorizer_layer.keras")

import tensorflow as tf
from tensorflow.keras.layers import (
    Input, Embedding, MultiHeadAttention, LayerNormalization,
    Dense, Dropout, GlobalAveragePooling1D
)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.regularizers import l2

# 1. –ö–ª–∞—Å—Å –¥–ª—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
class PositionalEmbedding(tf.keras.layers.Layer):
    def __init__(self, seq_length, embed_dim):
        super().__init__()
        self.seq_length = seq_length
        self.embed_dim = embed_dim
        self.pos_emb = tf.keras.layers.Embedding(
            input_dim=seq_length, output_dim=embed_dim
        )

    def call(self, x):
        positions = tf.range(start=0, limit=self.seq_length, delta=1)
        positions = tf.reshape(positions, (1, -1))
        pos_embeddings = self.pos_emb(positions)
        return x + pos_embeddings

# 2. Transformer Encoder Block
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):
    # Self-Attention
    x = MultiHeadAttention(
        num_heads=num_heads, key_dim=head_size
    )(inputs, inputs)
    x = Dropout(dropout)(x)
    x = LayerNormalization(epsilon=1e-6)(inputs + x)  # Residual connection

    # Feed Forward Network
    y = Dense(ff_dim, activation="gelu")(x)  # GELU –∫–∞–∫ –≤ BERT
    y = Dense(inputs.shape[-1])(y)
    y = Dropout(dropout)(y)
    return LayerNormalization(epsilon=1e-6)(x + y)

# 3. –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å
def build_transformer_model(
    max_tokens=100000,
    seq_length=600,
    embed_dim=128,
    num_heads=4,
    ff_dim=256,
    num_layers=4,
    dropout=0.1,
    l2_reg=1e-4
):
    inputs = Input(shape=(seq_length,))

    # Embedding + Positional
    x = Embedding(input_dim=max_tokens, output_dim=embed_dim)(inputs)
    x = PositionalEmbedding(seq_length, embed_dim)(x)

    # Transformer Blocks
    for _ in range(num_layers):
        x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout)

    # Classifier
    x = GlobalAveragePooling1D()(x)
    outputs = Dense(
        1,
        activation="sigmoid",
        kernel_regularizer=l2(l2_reg)
    )(x)

    return Model(inputs=inputs, outputs=outputs)

# 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
model = build_transformer_model(
    max_tokens=100000,
    seq_length=600,
    embed_dim=128,
    num_heads=4,
    ff_dim=512,
    num_layers=4,
    dropout=0.1
)

# 5. AdamW —Å –ª–∏–Ω–µ–π–Ω—ã–º –∑–∞—Ç—É—Ö–∞–Ω–∏–µ–º LR
total_steps = len(x_train) // 32 * 5  # 5 —ç–ø–æ—Ö
lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate=5e-5,
    end_learning_rate=1e-6,
    decay_steps=total_steps
)
optimizer = AdamW(learning_rate=lr_schedule, weight_decay=1e-4)

# 6. –ö–æ–º–ø–∏–ª—è—Ü–∏—è
model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# 7. –û–±—É—á–µ–Ω–∏–µ
history = model.fit(
    vectorized_train_text, y_train,
    epochs=5,
    batch_size=32,
    validation_data=(vectorized_test_text, y_test)
)

# 8. –û—Ü–µ–Ω–∫–∞
loss, accuracy = model.evaluate(vectorized_test_text, y_test)
print(f"Accuracy: {accuracy:.4f}")

import tensorflow as tf
from tensorflow.keras.layers import (
    Input, Embedding, MultiHeadAttention, LayerNormalization,
    Dense, Dropout, GlobalAveragePooling1D
)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.regularizers import l2

# 1. –ö–ª–∞—Å—Å –¥–ª—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
class PositionalEmbedding(tf.keras.layers.Layer):
    def __init__(self, seq_length, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.seq_length = seq_length
        self.embed_dim = embed_dim
        self.pos_emb = tf.keras.layers.Embedding(
            input_dim=seq_length, output_dim=embed_dim
        )

    def call(self, x):
        positions = tf.range(start=0, limit=self.seq_length, delta=1)
        positions = tf.reshape(positions, (1, -1))
        pos_embeddings = self.pos_emb(positions)
        return x + pos_embeddings

    def get_config(self):
        config = super().get_config()
        config.update({
            "seq_length": self.seq_length,
            "embed_dim": self.embed_dim
        })
        return config

# 2. Transformer Encoder Block
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):
    # Self-Attention
    x = MultiHeadAttention(
        num_heads=num_heads, key_dim=head_size
    )(inputs, inputs)
    x = Dropout(dropout)(x)
    x = LayerNormalization(epsilon=1e-6)(inputs + x)  # Residual connection

    # Feed Forward Network
    y = Dense(ff_dim, activation="gelu")(x)  # GELU –∫–∞–∫ –≤ BERT
    y = Dense(inputs.shape[-1])(y)
    y = Dropout(dropout)(y)
    return LayerNormalization(epsilon=1e-6)(x + y)

# 3. –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å
def build_transformer_model(
    max_tokens=100000,
    seq_length=600,
    embed_dim=128,
    num_heads=4,
    ff_dim=256,
    num_layers=4,
    dropout=0.1,
    l2_reg=1e-4
):
    inputs = Input(shape=(seq_length,))

    # Embedding + Positional
    x = Embedding(input_dim=max_tokens, output_dim=embed_dim)(inputs)
    x = PositionalEmbedding(seq_length, embed_dim)(x)

    # Transformer Blocks
    for _ in range(num_layers):
        x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout)

    # Classifier
    x = GlobalAveragePooling1D()(x)
    outputs = Dense(
        1,
        activation="sigmoid",
        kernel_regularizer=l2(l2_reg)
    )(x)

    return Model(inputs=inputs, outputs=outputs)

# 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
model = build_transformer_model(
    max_tokens=100000,
    seq_length=600,
    embed_dim=128,
    num_heads=4,
    ff_dim=512,
    num_layers=4,
    dropout=0.1
)

# 5. AdamW —Å –ª–∏–Ω–µ–π–Ω—ã–º –∑–∞—Ç—É—Ö–∞–Ω–∏–µ–º LR
total_steps = len(x_train) // 32 * 5  # 5 —ç–ø–æ—Ö
lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate=5e-5,
    end_learning_rate=1e-6,
    decay_steps=total_steps
)
optimizer = AdamW(learning_rate=lr_schedule, weight_decay=1e-4)

# 6. –ö–æ–º–ø–∏–ª—è—Ü–∏—è
model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# 7. –û–±—É—á–µ–Ω–∏–µ
history = model.fit(
    vectorized_train_text, y_train,
    epochs=5,
    batch_size=32,
    validation_data=(vectorized_test_text, y_test)
)

# 8. –û—Ü–µ–Ω–∫–∞
loss, accuracy = model.evaluate(vectorized_test_text, y_test)
print(f"Accuracy: {accuracy:.4f}")

import tensorflow as tf
from tensorflow.keras.layers import (
    Input, Embedding, MultiHeadAttention, LayerNormalization,
    Dense, Dropout, GlobalAveragePooling1D
)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.regularizers import l2

# 1. –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–ø—Ä—è–º–æ –≤ –º–æ–¥–µ–ª–∏)
def add_positional_embeddings(x, seq_length, embed_dim):
    positions = tf.range(start=0, limit=seq_length, delta=1)
    positions = tf.reshape(positions, (1, -1))  # (1, seq_length)
    pos_emb = tf.keras.layers.Embedding(input_dim=seq_length, output_dim=embed_dim)(positions)
    return x + pos_emb  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º

# 2. Transformer Encoder Block
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):
    # Self-Attention
    x = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)
    x = Dropout(dropout)(x)
    x = LayerNormalization(epsilon=1e-6)(inputs + x)  # Residual connection

    # Feed Forward Network
    y = Dense(ff_dim, activation="gelu")(x)  # GELU –∫–∞–∫ –≤ BERT
    y = Dense(inputs.shape[-1])(y)
    y = Dropout(dropout)(y)
    return LayerNormalization(epsilon=1e-6)(x + y)

# 3. –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å
def build_transformer_model(
    max_tokens=100000,
    seq_length=600,
    embed_dim=128,
    num_heads=4,
    ff_dim=256,
    num_layers=4,
    dropout=0.1,
    l2_reg=1e-4
):
    inputs = Input(shape=(seq_length,))

    # Embedding + –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    x = Embedding(input_dim=max_tokens, output_dim=embed_dim)(inputs)
    x = add_positional_embeddings(x, seq_length, embed_dim)  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏

    # Transformer Blocks
    for _ in range(num_layers):
        x = transformer_encoder(x, embed_dim, num_heads, ff_dim, dropout)

    # Classifier
    x = GlobalAveragePooling1D()(x)
    outputs = Dense(1, activation="sigmoid", kernel_regularizer=l2(l2_reg))(x)

    return Model(inputs=inputs, outputs=outputs)

# 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
model = build_transformer_model(
    max_tokens=100000,
    seq_length=600,
    embed_dim=128,
    num_heads=4,
    ff_dim=512,
    num_layers=4,
    dropout=0.1
)

# 5. AdamW —Å –ª–∏–Ω–µ–π–Ω—ã–º –∑–∞—Ç—É—Ö–∞–Ω–∏–µ–º LR
total_steps = len(x_train) // 32 * 5  # 5 —ç–ø–æ—Ö
lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate=5e-5,
    end_learning_rate=1e-6,
    decay_steps=total_steps
)
optimizer = AdamW(learning_rate=lr_schedule, weight_decay=1e-4)

# 6. –ö–æ–º–ø–∏–ª—è—Ü–∏—è
model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# 7. –û–±—É—á–µ–Ω–∏–µ
history = model.fit(
    vectorized_train_text, y_train,
    epochs=5,
    batch_size=32,
    validation_data=(vectorized_test_text, y_test)
)

loss, accuracy = model.evaluate(vectorized_test_text, y_test)
print(f"Accuracy: {accuracy:.4f}")

# 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
import keras
model.save('/content/drive/My Drive/fake_news_model.h5')

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞
keras.models.save_model(vectorizer, 'text_vectorizer_layer.keras')

model.save('/content/drive/My Drive/fake_news_model.keras')

vocab = vectorizer.get_vocabulary()

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
with open("vocab.txt", "w", encoding="utf-8") as f:
    for token in vocab:
        f.write(token + "\n")

model.save("/content/drive/My Drive/NWfake_news_model_fixed.h5")

model.save_weights("/content/drive/My Drive/NEW.weights.h5")

#–¥–ª—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
model.save('/content/drive/My Drive/fake_news_model.h5')

tt=pd.read_csv("tt.csv")
tt



import spacy

en = spacy.load('en_core_web_sm')
def lemmatize(text):
    a = en(text)
    return ' '.join([token.lemma_ for token in a if
                  not token.is_stop])

r = tt['text'].apply(lemmatize)
r=r.apply(wordopt)
r
#vectorized_test_text = vectorizer(.tolist())

vectorized_test_text = vectorizer(r.tolist())
y_test=[1]
yt=np.reshape(y_test, (-1, 1))

loss, accuracy = model.evaluate(vectorized_test_text, yt)
print(f"Accuracy: {accuracy:.4f}")

import keras

keras.models.save_model(vectorizer, 'text_vectorizer_layer.keras')

"""bert"""

import tensorflow as tf
import numpy as np
from tqdm import tqdm
from transformers import TFBertModel, BertTokenizer
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling1D, MultiHeadAttention, LayerNormalization
from tensorflow.keras.optimizers import Adam

bert_model = TFBertModel.from_pretrained("bert-base-uncased")
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
bert_model.trainable = False  # –ó–∞–º–æ—Ä–æ–∑–∏–º –≤–µ—Å–∞

def tokenize_and_save(texts, labels, tokenizer, max_len, save_path="tokenized_data.npz"):
    input_ids = []
    attention_masks = []

    for text in tqdm(texts, desc="Tokenizing"):
        tokens = tokenizer(
            text,
            padding='max_length',
            truncation=True,
            max_length=max_len,
            return_tensors='tf'
        )
        input_ids.append(tokens['input_ids'][0])
        attention_masks.append(tokens['attention_mask'][0])

    input_ids = tf.stack(input_ids)
    attention_masks = tf.stack(attention_masks)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
    np.savez_compressed(
        save_path,
        input_ids=input_ids.numpy(),
        attention_mask=attention_masks.numpy(),
        labels=np.array(labels)
    )

    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ —Ñ–∞–π–ª: {save_path}")
    return input_ids, attention_masks, labels

# 5. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
max_len = 64
train_encodings, _, y_train = tokenize_and_save(x_train.tolist(), y_train, tokenizer, max_len, save_path="train_tokenized.npz")
test_encodings, _, y_test = tokenize_and_save(x_test.tolist(), y_test, tokenizer, max_len, save_path="test_tokenized.npz")
test_encodings, _, y_test = tokenize_and_save(x_val.tolist(), y_val, tokenizer, max_len, save_path="val_tokenized.npz")

from tensorflow.keras.layers import Layer
from transformers import TFBertModel
class BertEmbedding(Layer):
    def __init__(self, model_name="bert-base-uncased", **kwargs):
        super().__init__(**kwargs)
        self.bert = TFBertModel.from_pretrained(model_name)

    def call(self, inputs):
        return self.bert(inputs).last_hidden_state

max_len = 64
def load_tokenized(load_path="tokenized_data.npz"):
    data = np.load(load_path)
    input_ids = tf.convert_to_tensor(data['input_ids'])
    attention_mask = tf.convert_to_tensor(data['attention_mask'])
    labels = data['labels']
    print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ —Ñ–∞–π–ª–∞: {load_path}")
    return input_ids, attention_mask, labels
# 6. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
train_input_ids, train_attention_mask, y_train = load_tokenized("train_tokenized.npz")
test_input_ids, test_attention_mask, y_test = load_tokenized("test_tokenized.npz")
val_input_ids, val_attention_mask, y_val = load_tokenized("val_tokenized.npz")

# 7. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
def build_model(max_len, num_heads=4, ff_dim=256, num_layers=2, dropout=0.1):
    input_ids = Input(shape=(max_len,), dtype=tf.int32, name="input_ids")
    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name="attention_mask")

    # –ö–∞—Å—Ç–æ–º–Ω—ã–π —Å–ª–æ–π ‚Äî —Ç–µ–ø–µ—Ä—å –æ–±—Ö–æ–¥–∏—Ç –æ—à–∏–±–∫—É —Å KerasTensor
    x = BertEmbedding()({"input_ids": input_ids, "attention_mask": attention_mask})

    for _ in range(num_layers):
        x = transformer_encoder(x, head_size=64, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout)

    x = GlobalAveragePooling1D()(x)
    x = Dropout(dropout)(x)
    output = Dense(1, activation="sigmoid")(x)

    model = Model(inputs=[input_ids, attention_mask], outputs=output)
    return model




# 8. –ö–æ–º–ø–∏–ª—è—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = build_model(max_len=max_len)

model.compile(
    optimizer=Adam(learning_rate=2e-5),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# 9. –û–±—É—á–µ–Ω–∏–µ


model.fit(
    x={
        'input_ids': train_input_ids,
        'attention_mask': train_attention_mask
    },
    y=y_train,
    validation_data=(
        {
            'input_ids': test_input_ids,
            'attention_mask': test_attention_mask
        },
        y_test
    ),
    epochs=3,
    batch_size=32
)

model.save('/content/drive/My Drive/bert_model.keras')

model.save("/content/drive/My Drive/NWfake_news_model_fixed.h5")

# 10. –û—Ü–µ–Ω–∫–∞
#qwerty
loss, acc = model.evaluate(
    {
        'input_ids': val_input_ids,
        'attention_mask': val_attention_mask
    },
    y_val
)
print(f"Test Accuracy: {acc:.4f}")

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# 1. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ
y_pred_probs = model.predict(
    {
        'input_ids': val_input_ids,
        'attention_mask': val_attention_mask
    },
    batch_size=32
)

# 2. –û–∫—Ä—É–≥–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ –∫–ª–∞—Å—Å—ã (0 –∏–ª–∏ 1)
y_pred = (y_pred_probs.flatten() > 0.5).astype(int)

# 3. –í—ã–≤–æ–¥–∏–º –æ—Ç—á–µ—Ç –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º
print(classification_report(y_val, y_pred, digits=4))

# 4. –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
cm = confusion_matrix(y_val, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Fake', 'Real'])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

model.fit(
    x={
        'input_ids': train_input_ids,
        'attention_mask': train_attention_mask
    },
    y=y_train,
    validation_data=(
        {
            'input_ids': test_input_ids,
            'attention_mask': test_attention_mask
        },
        y_test
    ),
    epochs=3,
    batch_size=32
)

import pandas as pd

# –°–æ–∑–¥–∞—ë–º DataFrame
df = pd.DataFrame({
    "text": ["–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ"],
    "label": [1]
})

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
df.to_csv("example.csv", index=False, encoding="utf-8-sig")

print("‚úÖ –§–∞–π–ª 'example.csv' —Å–æ–∑–¥–∞–Ω.")

df.iloc[0].text=tt['text'].iloc[0]

df

tt

import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.layers import Layer, Input, Dense, Dropout, GlobalAveragePooling1D, MultiHeadAttention, LayerNormalization
from transformers import TFBertModel
import numpy as np
# –ö–∞—Å—Ç–æ–º–Ω—ã–π —Å–ª–æ–π —Å BERT
@tf.keras.utils.register_keras_serializable()
class BertEmbedding(Layer):
    def __init__(self, model_name="bert-base-uncased", **kwargs):
        super().__init__(**kwargs)
        self.bert = TFBertModel.from_pretrained(model_name)

    def call(self, inputs):
        return self.bert(inputs).last_hidden_state

# –ö–∞—Å—Ç–æ–º–Ω—ã–π encoder –±–ª–æ–∫
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):
    x = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)
    x = Dropout(dropout)(x)
    x = LayerNormalization(epsilon=1e-6)(inputs + x)  # residual

    y = Dense(ff_dim, activation="gelu")(x)
    y = Dense(inputs.shape[-1])(y)
    y = Dropout(dropout)(y)
    return LayerNormalization(epsilon=1e-6)(x + y)

model = load_model(
    '/content/drive/My Drive/bert_model.keras',
    custom_objects={
        'BertEmbedding': BertEmbedding,
        'transformer_encoder': transformer_encoder
    }
)

model.save("/content/drive/My Drive/final_model.keras")

model.save('/content/drive/My Drive/newfinal_model.keras', save_format='keras')

def tokenize(texts, labels, tokenizer, max_len):
    input_ids = []
    attention_masks = []

    for text in texts:
        tokens = tokenizer(
            text,
            padding='max_length',
            truncation=True,
            max_length=max_len,
            return_tensors='np'  # ‚¨ÖÔ∏è –≤–º–µ—Å—Ç–æ 'tf'
        )
        input_ids.append(tokens['input_ids'][0])
        attention_masks.append(tokens['attention_mask'][0])

    input_ids = np.stack(input_ids)
    attention_masks = np.stack(attention_masks)
    labels = np.array(labels)

    return input_ids, attention_masks, labels

tt=pd.read_csv("t2.csv")
tt

test_input_ids, test_attention_mask, y_test = tokenize(tt['text'].tolist(), tt['label'].tolist(), tokenizer, max_len=512)
loss, acc = model.evaluate(
    x={
        'input_ids': test_input_ids,
        'attention_mask': test_attention_mask
    },
    y=y_test
)
print(f"‚úÖ Accuracy: {acc:.4f}")

max_len = 512
def load_tokenized(load_path="tokenized_data.npz"):
    data = np.load(load_path)
    input_ids = tf.convert_to_tensor(data['input_ids'])
    attention_mask = tf.convert_to_tensor(data['attention_mask'])
    labels = data['labels']
    print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ —Ñ–∞–π–ª–∞: {load_path}")
    return input_ids, attention_mask, labels
test_input_ids, test_attention_mask, y_test = load_tokenized("/content/drive/My Drive/test_tokenized.npz")
loss, acc = model.evaluate(
    x={
        'input_ids': test_input_ids,
        'attention_mask': test_attention_mask
    },
    y=y_test
)
print(f"‚úÖ Accuracy: {acc:.4f}")

